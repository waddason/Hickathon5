{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "X_train = pd.read_csv('data/X_train_ready.csv')\n",
    "X_test = pd.read_csv('data/X_test_ready.csv')\n",
    "y_train = pd.read_csv('data/y_train_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the rows that are in X\n",
    "y_train = y_train[y_train[\"row_index\"].isin(X_train[\"row_index\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelle colonne est manquante dans X_train ?\n",
    "print(\"Columns missing in X_train: \", set(X_test.columns) - set(X_train.columns))\n",
    "print(\"Columns missing in X_test: \", set(X_train.columns) - set(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns=['piezo_measurement_date.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "\n",
    "# Vérification des NaN\n",
    "\n",
    "print(\"X_train NaN: \", X_train.isnull().sum().sum())\n",
    "print(\"X_test NaN: \", X_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplifier les colonnes à conserver\n",
    "columns_to_keep = [\n",
    "    'piezo_station_bss_id', 'piezo_station_altitude', 'piezo_station_longitude', 'piezo_station_latitude',\n",
    "    'piezo_measurement_date', 'hydro_delta_7d', 'hydro_delta_30d', 'hydro_delta_90d',\n",
    "    'meteo_rain_height', 'meteo_snow_height', 'row_index'\n",
    "]\n",
    "\n",
    "# Filtrer les colonnes dans X_train et X_test\n",
    "X_train = X_train[columns_to_keep]\n",
    "X_test = X_test[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normaliser les colonnes continues\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.drop(columns=['piezo_measurement_date', 'row_index','piezo_station_bss_id']))\n",
    "X_test = scaler.transform(X_test.drop(columns=['piezo_measurement_date', 'row_index','piezo_station_bss_id']))\n",
    "\n",
    "# Préparer les cibles pour la classification (assurez-vous que y_train est au format entier)\n",
    "y_train = y_train['piezo_groundwater_level_category'].astype(int)\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the data \n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normaliser les colonnes continues\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Préparer les cibles pour la classification (assurez-vous que y_train est au format entier)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la longueur des séquences\n",
    "sequence_length = 30  # Par exemple, 30 jours\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train).flatten()\n",
    "y_val = np.array(y_val).flatten()\n",
    "\n",
    "print(f\"y_train (après conversion): {y_train.shape}\")\n",
    "print(f\"y_val (après conversion): {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    if len(data) <= sequence_length:\n",
    "        raise ValueError(f\"La longueur des données ({len(data)}) est insuffisante pour créer des séquences de longueur {sequence_length}.\")\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i + sequence_length])\n",
    "        targets.append(target[i + sequence_length])\n",
    "    return np.array(sequences), np.array(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifiez que tout est conforme\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val_scaled shape: {X_val_scaled.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "\n",
    "# Longueur des séquences\n",
    "sequence_length = 30\n",
    "\n",
    "# Génération des séquences\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train, sequence_length)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val, sequence_length)\n",
    "\n",
    "# Vérifiez les dimensions des séquences générées\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq shape: {X_val_seq.shape}\")\n",
    "print(f\"y_val_seq shape: {y_val_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le modèle LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))  # Régularisation\n",
    "model.add(LSTM(30, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))  # Une seule sortie pour la régression\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=20,  # Nombre d'époques (ajustez selon les performances)\n",
    "    batch_size=32,  # Taille des lots\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Évaluation sur le jeu de validation\n",
    "val_predictions = model.predict(X_val_seq)\n",
    "mse = mean_squared_error(y_val_seq, val_predictions)\n",
    "print(f\"Mean Squared Error on Validation Set: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=50,  # Nombre d'époques\n",
    "    batch_size=32,  # Taille des lots\n",
    "    validation_data=(X_val_seq, y_val_seq),  # Jeu de validation\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Évaluation du modèle\n",
    "y_val_pred = model.predict(X_val_seq)\n",
    "mse = mean_squared_error(y_val_seq, y_val_pred)\n",
    "mae = mean_absolute_error(y_val_seq, y_val_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "# Visualisation des pertes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Loss (Entraînement)')\n",
    "plt.plot(history.history['val_loss'], label='Loss (Validation)')\n",
    "plt.title('Évolution de la perte au fil des époques')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Tracer les prédictions par rapport aux vraies valeurs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_val_seq[:200], label='Vraies valeurs', linestyle='--', marker='o', alpha=0.7)\n",
    "plt.plot(y_val_pred[:200], label='Prédictions', linewidth=2, alpha=0.7)\n",
    "plt.title('Comparaison entre les vraies valeurs et les prédictions')\n",
    "plt.xlabel('Échantillons')\n",
    "plt.ylabel('Niveau de la nappe')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(50, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submission = xgb_model.predict(dtest)\n",
    "\n",
    "# Adjust the target classes to start from 1\n",
    "y_submission = y_submission + 1\n",
    "print(y_submission[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = pd.Series(y_submission).map(target_level)\n",
    "\n",
    "print(y_target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_filtered.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame([X_test_filtered[\"row_index\"], y_target]).T\n",
    "submission.columns = [\"row_index\", \"piezo_groundwater_level_category\"]\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_g06_02_xgb.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
