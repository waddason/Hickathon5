{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDZzrelKxnpx"
   },
   "source": [
    "**<center><font color='#023F7C' size=\"6.5\">AI Explainability course</font>** <br>\n",
    "<font color=#023F7C size=4>**Hi!ckathon #5 - AI & Sustainability**</font> <br>\n",
    "<font color=#023F7C size=2> 6:15PM-6:45PM </font> <br>\n",
    "\n",
    "</center>\n",
    "\n",
    "<img src = https://www.hi-paris.fr/wp-content/uploads/2020/09/logo-hi-paris-retina.png width = \"300\" height = \"200\" >\n",
    "\n",
    "<font color=\"#023F7C\">**Author**:</font> Laurène DAVID, Machine Learning Research Engineer @ Hi! PARIS <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the Data Science courses are available in the <font color=#023F7C><b> Hickathon5 git repository</b></font>:  https://github.com/hi-paris/Hickathon5 <br>\n",
    "If you are enjoying these courses, please don't hesitate to add a star ⭐ to this repository. The team would greatly appreciate it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7813,
     "status": "ok",
     "timestamp": 1732204314524,
     "user": {
      "displayName": "laurene david",
      "userId": "12504925674923465673"
     },
     "user_tz": -60
    },
    "id": "IpaRRiDNSilx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732204314524,
     "user": {
      "displayName": "laurene david",
      "userId": "12504925674923465673"
     },
     "user_tz": -60
    },
    "id": "oDDO_68DCIsi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cznGfEA-XIDN"
   },
   "source": [
    "# **1. Explainable AI**\n",
    "\n",
    "**Explainable AI** refers to the ability of a Machine Learning model to deliver human understandable explanations for the predictions made by a machine. It's goal is to bridge the gap between human understanding and AI models by making the decision-making process interpretable and transparent.\n",
    "\n",
    "Understanding the inner workings of a model helps build trust among users\n",
    "and stakeholders, as well as increase acceptance. This is crucial, especially in sensitive domains like healthcare, finance, and criminal justice with ethical concerns and regulation requirements.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = https://www.darpa.mil/ddm_gallery/xai-figure2-inline-graphic.png width = \"600\" height = \"350\" >\n",
    "\n",
    "In practice, Explainable AI means being able to **understand which feature used by a model during training had the most impact on the final predictions** and how to **quantify this impact**.\n",
    "\n",
    "Most machine learning models have their own method to quantify feature importance. Here is a recap of these methods.\n",
    "- **Linear Regression, Logistic Regression, SVM** (Linear models): the absolute value of the estimated coefficients can be seen as a measure of feature importance\n",
    "- **Decision Tree, Random Forest, XGBoost** (Tree models): Impurity-based methods for explainability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EKt-apGwVgT"
   },
   "source": [
    "## Explainability vs Performance\n",
    "It is important to note that not every model is equally explainable. <br> There is often a trade-off between explainability and model performance.\n",
    "\n",
    "<img src = https://www.researchgate.net/publication/332209054/figure/fig1/AS:743991452631045@1554392793570/Accuracy-vs-Explainability-of-the-main-machine-learning-algorithms.ppm width = \"600\" height = \"400\" >\n",
    "\n",
    "Neural networks are considered to be state-of-the-art models for many tasks in Machine Learning but they are the least explainable, understandable to humans. They are often categorized as \"Black-Box\" models since it can be very difficult to understand their decision making process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCZPvxvNhOrc"
   },
   "source": [
    "## Model-specific explainability (Random Forest)\n",
    "\n",
    "Here is a reminder of a model-specific explainability method using a Random Forest model. <br>\n",
    "\n",
    "\n",
    "`RandomForestClassifier` has an attribute called `.feature_importances_` which uses **impurity-based feature importance** to provide explainability. Impurity-based explainability measures the importance of a feature by looking at the decrease in impurity that a feature introduces when used for splitting nodes in the tree.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = https://miro.medium.com/v2/resize:fit:781/1*fGX0_gacojVa6-njlCrWZw.png width = \"500\" height = \"300\" >\n",
    "\n",
    "These methods are specific to Machine Learning models that uses trees, such as Decision Tree, Random Forest and Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "error",
     "timestamp": 1732204315005,
     "user": {
      "displayName": "laurene david",
      "userId": "12504925674923465673"
     },
     "user_tz": -60
    },
    "id": "JokumubGjBq0",
    "outputId": "a0d537d2-6842-4105-a69f-6250293c7b5f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_data = \"..\\Machine_Learning\"\n",
    "data_class_train = pd.read_csv(os.path.join(path_data,\"CoursML_Classification_train.csv\"))\n",
    "data_class_test = pd.read_csv(os.path.join(path_data, \"CoursML_Classification_test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1732204315005,
     "user": {
      "displayName": "laurene david",
      "userId": "12504925674923465673"
     },
     "user_tz": -60
    },
    "id": "SrRac7ZHT03k",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Get train-test split with X and y\n",
    "X_train, y_train = data_class_train.drop(columns=[\"genre\"]), data_class_train[\"genre\"]\n",
    "X_test, y_test = data_class_test.drop(columns=[\"genre\"]), data_class_test[\"genre\"]\n",
    "\n",
    "# Train a random forest model\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict values for X_test\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Compute the predictions accuracy\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred_rf)\n",
    "print(\"Model accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9039,
     "status": "aborted",
     "timestamp": 1732204315005,
     "user": {
      "displayName": "laurene david",
      "userId": "12504925674923465673"
     },
     "user_tz": -60
    },
    "id": "-kYVtScqU7w8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance_rf = pd.DataFrame({\"feature\":X_train.columns, \"score\":rf.feature_importances_})\n",
    "feature_importance_rf.sort_values(by=[\"score\"], ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(data= feature_importance_rf, x=\"score\", y=\"feature\")\n",
    "plt.title(\"Feature importance with Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MVgNyyhhna1"
   },
   "source": [
    "**Disadvantages**: Impurity-based feature importance tend to be strongly biased and favor high cardinality features (typically numerical features) over low cardinality features such as binary or categorical variables. This method can also give a high importance to features used on an overfitted model. These features may not be predictive on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ujiyL_LPEbY"
   },
   "source": [
    "# **2. Explainability with SHAP**\n",
    "\n",
    "**SHAP** (SHapley Additive exPlanations) is a model-agnostic method to explain the predictions of a Machine Learning model. <br> It computes the shapely values of each feature to measure its importance, which is a concept taken from Game Theory.\n",
    "\n",
    "- In Game theory, shapley values help determine how much each player in a collaborative game has contributed to the total payout (prediction in our case). For a machine learning model, each feature is considered a \"player\". The Shapley value for a feature represents the **average magnitude of that feature's contribution** across all possible combinations of features.\n",
    "\n",
    "- SHAP gives the contribution of each feature to the individual predictions (on a sample level). <br> It is thus a **local method for explainability**, even though global importance can be computed by averaging the contributions per feature (an example will be shown later).\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = https://shap.readthedocs.io/en/latest/_images/shap_header.png width = \"600\" height = \"350\" >\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP measures the importance of each feature by comparing the predictions made by the model to a **baseline value**. <br> A baseline value is a predicted by a model with no knowledge of any features and is computed by averaging the target values.\n",
    "- Each shap values represents the difference between the baseline and the predictions\n",
    "- A negative shap values doesn't mean the feature had no impact on the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpNboNc7CIsk"
   },
   "source": [
    "**About SHAP's explainers** <br>\n",
    "The SHAP package has many types of explainer functions to compute the Shapley values of a given Machine Learning model. <br>\n",
    "Here are a few important ones:\n",
    "- `shap.Explainer`: Generic explainer function that can be used on any model (including black-box models such as Neural Networks).\n",
    "- `shap.TreeExplainer`: Fast explainer that can only be used for tree-based models, such as Decision Trees, Random Forests and Boosting models\n",
    "- `shap.DeepExplainer`: Explainer designed for deep learning models\n",
    "\n",
    "You can find the full list of shap explainers [here](https://shap.readthedocs.io/en/latest/api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxIympxQj8FP",
    "outputId": "d7829e78-9aae-4e0a-e388-0fdd80a4a673",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install shap --quiet --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2U-WT0DH_Vy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O84X6KGzlAt"
   },
   "source": [
    "## Compute shap values\n",
    "\n",
    "Let's use SHAP on our previously trained **Random Forest model** to explain the track genre predictions. <br>\n",
    "We will use the package's `TreeExplainer` to compute shap values, since our model is tree-based.\n",
    "\n",
    "For **multiclass classification**, SHAP computes the shap values for each predicted label. In our case, we have six genres to predict. <br> The `shap_values` object contains the computed shap values for every specific genre.\n",
    "\n",
    "<img src = https://miro.medium.com/v2/resize:fit:1090/1*9LorUdFygbU4vtQRqiO_5g.png width = \"550\" height = \"300\" >\n",
    "\n",
    "In this instance, we are evaluating the importance of each feature in the predictions of the **training set.** <br>\n",
    "To explain the predictions on the test set, you must compute the model's shap values with `X_test`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3KzCtn5jdax",
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "8EVaZBNqOg5d",
    "outputId": "a53919ed-38bc-4ce2-fa55-45faf06c734e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the shap values on the training set\n",
    "np.bool = bool\n",
    "\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Shap values for the classical genre (index 0)\n",
    "pd.DataFrame(shap_values[:,:,0], columns=X_train.columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unT2O4v50-z1"
   },
   "source": [
    "<u>**Base value**</u>: SHAP measures the importance of each feature by comparing the predictions made by the model to a \"baseline value\". <br>\n",
    "A baseline value is a predicted by a model with no knowledge of any features and is computed by averaging the target values.\n",
    "- Each shap values represents the difference between the baseline and the predictions\n",
    "- A negative shap values doesn't mean the feature had no impact on the final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGqW3RC5OTNj"
   },
   "source": [
    "## Global feature importance\n",
    "The computed shap values gives the importance of a feature on a sample level. <br> To get the global feature importance, we need to compute the **average shap values** for each feature across all possible classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average SHAP values across classes (for multi-class classification)\n",
    "shap_values_mean = np.abs(shap_values).mean(axis=2)\n",
    "\n",
    "# Plot global feature importance for all classes\n",
    "plt.title(\"Global feature importance for all classes\", fontsize=15)\n",
    "shap.summary_plot(shap_values_mean, X_train, feature_names=X_train.columns, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the feature importance for each individual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = [\"blue\", \"green\", \"red\", \"orange\", \"yellow\", \"pink\"]\n",
    "\n",
    "for idx, cl in enumerate(rf.classes_):\n",
    "    shap.summary_plot(shap_values[:,:,idx], X_train, feature_names=X_train.columns, plot_type=\"bar\", color=colors[idx], show=False)\n",
    "    plt.title(\"Global importance for the {cl} genre\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mApjC7F_p-fz"
   },
   "source": [
    "## Local feature importance\n",
    "To understand how a feature impacted the final predictions, we will use `.summary_plot()` with `plot_type=\"dot\"`. <br>\n",
    "In the following example, we will explain the predictions for only the <u>classical</u> case since shap doesn't allow multiclass output with this type of plot.\n",
    "\n",
    "The following graph shows the **impact (positive or negative) of a feature for a single predicted point**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "ogs2id5ssM6F",
    "outputId": "21c1840b-d9c9-4536-d486-36b2e6e19a2d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[:,:,0], X_train, feature_names=X_train.columns, plot_type=\"dot\", show=False) # show=False to be able to add title\n",
    "plt.title(\"Feature importance for the classical genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tws9j1Gs1Mgi"
   },
   "source": [
    "In this graph, the x-axis represents the shap value, and the y-axis represents the features. \n",
    "- Each point on the chart is a single value for a prediction and feature. \n",
    "- Red color means higher value of a feature. \n",
    "- Blue means lower value of a feature.\n",
    "\n",
    "\n",
    "**How to interpret this graph ?** <br>\n",
    "- Songs with a higher acousticness level are more likely to be classified as \"classical\" by the model\n",
    "- Songs with a higher loudness value tend to be classified more often as \"classical\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3GYvOBxkfbG"
   },
   "source": [
    "# **3. Explainability with Permutation Importance and Partial Dependance plot**\n",
    "\n",
    "SHAP can be difficult to use on large datasets since the computation of shap values can be very slow. <br>\n",
    "In this section, we will show you two \"faster\" model-agnostic methods to compute feature importance using `sklearn`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0IjKlNyX4Jk"
   },
   "source": [
    "## Permutation Importance\n",
    "\n",
    "**Permutation feature importance** computes its score by measuring the decrease in a model's performance when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = https://www.aporia.com/wp-content/webp-express/webp-images/uploads/2022/09/Untitled-1100-%C3%97-650-px-1024x605.png.webp width = \"600\" height = \"350\" >\n",
    "\n",
    "<u> Note </u>: Features that are deemed of low importance for a bad model could be very important for a good model. Permutation importance does not reflect to the importance of a feature by itself but how important this feature is for a particular model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvMA8xczjB8P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Compute permutation importance for each feature using an accuracy score\n",
    "permutation_score = permutation_importance(rf, X_train, y_train, n_repeats=30, scoring=\"accuracy\", random_state=0) # n_repeats: number of times data is reshuffled\n",
    "\n",
    "# Create dataframe\n",
    "permutation_df = pd.DataFrame({\"feature\":X_train.columns,\n",
    "                               \"mean score\":permutation_score.importances_mean,\n",
    "                               \"std\":permutation_score.importances_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "iKLeskejlzyZ",
    "outputId": "c23953b7-9d89-40cd-bd16-1a1a578c6f9c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "permutation_df.sort_values(by=[\"mean score\"], ascending=False, inplace=True)\n",
    "permutation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "rr5ES9p2nXmO",
    "outputId": "de582770-1b20-4b22-92d6-eee989f11aee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(data = permutation_df, x=\"mean score\", y=\"feature\")\n",
    "plt.title(\"Permutation feature importance with Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygX7FyTcYWNh"
   },
   "source": [
    "## Partial Dependance plot\n",
    "\n",
    "**Partial dependence plots** shows the relationship between a feature and the predictions of a model while keeping other features constant. It can show whether the relationship between the target and a feature is linear, monotonic or more complex.\n",
    "- It is a useful tool for interpreting the effects of individual features on the model's predictions without having use the `shap` package.\n",
    "- These plots can only be used to explain the impact of one to two features, so it can be useful to first select which features to study using other explainability methods.\n",
    "\n",
    "**How is partial dependence computed ?**\n",
    "- Start by creating multiple versions of the chosen feature with different values\n",
    "- Duplicate the training dataset by only modifying the values of the chosen feature\n",
    "- Train a Machine Learning model on each duplicate datasets then aggregate the target value for each\n",
    "- Plot the aggregated target value based on the value of the chosen feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "kLqBZ-U2ZuIq",
    "outputId": "bd294f5a-6435-42c6-a694-db8b513c67a5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "selected_features = [\"acousticness\", \"loudness\"]\n",
    "PartialDependenceDisplay.from_estimator(rf, X_train, features=selected_features, target=\"classical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zE5gXzPknAwZ"
   },
   "source": [
    "**How to interpret this graph ?**\n",
    "- A higher acousticness level tends to increase the target value (the target gets closer to 1, which is when the track's genre is classical)\n",
    "- A lower danceability level tends to decrease the target value (the target gets closer to 0, which is when the track's genre is not classical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25hwujuIioYr"
   },
   "source": [
    "You can also create a partial dependance plot using two features. <br>\n",
    "It can show the impact of a combination of these features on the final predictions.\n",
    "\n",
    "**Note**: Using two features to create a partial dependance plot can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "ulpgTkGIfNIz",
    "outputId": "709d603f-ecc4-4d8e-f2e6-0705f3ca604e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_features = [(\"acousticness\", \"loudness\")]\n",
    "PartialDependenceDisplay.from_estimator(rf, X_train, features=selected_features, target=\"classical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOsIvKf0H6qv"
   },
   "source": [
    "**How to interpret this graph ?** <br>\n",
    "Songs with a higher acousticness level and a lower loudness level were more often predicted as classical by the model.\n",
    "- `Acousticness` has a \"positive\" impact on predictions (increases the chance of being predicted 1)\n",
    "- `Loudness` has a \"negative\" impact on predictions (decreases the change of being predicted 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Other packages/frameworks**\n",
    "\n",
    "Many python packages/frameworks exist to compute model explainability. <br>\n",
    "Here are few you can also check out:\n",
    "- **Shapash**: Python library based on SHAP, designed to build user-friendly interactive dashboards for machine learning model explainability. (https://shapash.readthedocs.io/en/latest/)\n",
    "- **LIME** (Local Interpretable Model-agnostic Explanations): Explain individual predictions by fitting simple interpretable models (e.g., linear models) locally around the prediction (https://github.com/marcotcr/lime?tab=readme-ov-file)\n",
    "- **Captum**: PyTorch library for Deep Learning model explainability. It offers gradient-based attribution methods like Integrated Gradients, Saliency Maps, and DeepLIFT.\n",
    "- **TF-Explain**: A TensorFlow-based library providing tools for visualizing Grad-CAM, Saliency Maps, and other interpretability techniques for neural networks.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example of a Shapash app**\n",
    "\n",
    "<img src =\n",
    "https://miro.medium.com/v2/resize:fit:720/format:webp/1*oaJTad5H3PZk5oUNrcoXag.gif width = \"800\" height = \"450\" >\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example of Grad-CAM explainability on images**\n",
    "\n",
    "<img src=https://datascientest.com/en/files/2021/07/Original-Image.jpg width = \"500\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hi! PARIS Engineering Team also developed a package for AI Explainability called **XPER**. <br>\n",
    "You can check it out here: https://github.com/hi-paris/XPER/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5ZBJWNKCIsr"
   },
   "source": [
    "<font size='5'>**The Explainability course ends here, thank you for listening !** </font><br>\n",
    "This notebook is available on HFactory and Github."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_hi5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
