{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0djk4hDwjbRG"
   },
   "source": [
    "**<center><font color='#023F7C' size=\"6.5\">Course 1: Feature Engineering </font>** <br>\n",
    "<font color=#023F7C size=4>**Hi!ckathon #5 - AI & Sustainability**</font> <br>\n",
    "<font color=#023F7C size=2> 6:15PM-8:00PM </font> <br>\n",
    "\n",
    "</center>\n",
    "\n",
    "<img src = https://www.hi-paris.fr/wp-content/uploads/2020/09/logo-hi-paris-retina.png width = \"300\" height = \"200\" >\n",
    "\n",
    "<font color=\"#023F7C\">**Author**:</font> Laur√®ne DAVID, Machine Learning Research Engineer @ Hi! PARIS <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhA6NqJkTZUO"
   },
   "source": [
    "# **Pre-Hi!ckathon training** üöÄ\n",
    "\n",
    "The Hi!ckathon is a competitive AI and Data Science challenge organized by **Hi! PARIS**, the Center dedicated to data science and AI. The center was created by **HEC Paris** and **Institut Polytechnique de Paris** in September 2020 with Inria joining in 2021.\n",
    "\n",
    "This 5th edition will take place in several stages:\n",
    "- First, a **preparation phase**, starting on November 12 with three Data Science courses (Feature engineering, Machine Learning, Deep Learning & Explainability) and a pitch course.\n",
    "- A **Career Fair** will be held on November 14 starting at 6:30 PM, at T√©l√©com Paris.\n",
    "- The **final sprint** on the weekend of November 29 to December 2.\n",
    "\n",
    "\n",
    "For more information, you can visit the Hi!ckathon's official [website](https://www.hi-paris.fr/hickathon/).\n",
    "\n",
    "<img src = https://www.hi-paris.fr/wp-content/uploads/2024/10/Hickathon-5-JourneyTemplate_FondBlanc-1536x864.jpg width = \"800\" height = \"500\" >\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw7pQRQxpMo-"
   },
   "source": [
    "**<u>About the Data Science courses</u>** <br>\n",
    "These three courses will be provided by Hi! PARIS' Engineering Team. <br> Their goal is to **refresh your memory on some key Data Science concepts** and help you prepare for the Hi!ckathon's final sprint.\n",
    "\n",
    "In the first course, we will focus on the first steps of a Data Science project: **Data Cleaning**, **Data Exploration** and **Feature Engineering**. <br> These steps insure you get the best possible outcome when training a Machine Learning model. They are essential steps in a Data Science project timeline.\n",
    "\n",
    "\n",
    "<img src = https://miro.medium.com/v2/resize:fit:1400/1*RiEfzta7FkYiRq44jhSb7g.png width = \"650\" height = \"500\" >\n",
    "\n",
    "\n",
    "In the following two courses, you will learn how to build Machine Learning and Deep Learning models, how to test their performance and how to improve them. At the end of the courses, we will also give a brief introduction to Explainability in Machine Learning.\n",
    "- November 21, 2024 (6:15PM - 8:00PM): **Machine Learning**\n",
    "- November 26, 2024 (6:15PM - 8:00PM): **Deep Learning and Explainability**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIDdn35GJnFY"
   },
   "source": [
    "**<u>Github repository</u>** <br>\n",
    "All of the courses are also available in the **Hickathon5** github repository <bR>https://github.com/hi-paris/Hickathon5\n",
    "\n",
    "*If you are enjoying these courses, please don't hesitate to add a star ‚≠ê to this repository.* <br>\n",
    "*The team would greatly appreciate it !*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZwqzG62TLv_"
   },
   "source": [
    "**<u>Need help ?</u>** <bR>\n",
    "In these courses, we won't cover how to code in Python and use its Data Science libraries. <br> If you do need help with these subjects, you can visit the **Additional resources** section on HFactory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQKk25eHGoRw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzzB8jeOvRzv"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODiFovuNP1z4"
   },
   "source": [
    "# **I. Data Cleaning üßπ**\n",
    "\n",
    "Data Cleaning is the first step of any Data Science project and can sometimes be very time consuming. <br>\n",
    "It involves identifying and correcting errors, inconsistencies or incomplete data within a dataset.\n",
    "- Handle missing values\n",
    "- Remove duplicates/irrelevant or hard to preprocess variables\n",
    "- Clean text data with an incorrect format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvaVb2ezXxrg"
   },
   "source": [
    "**Why is it important ?** <br>\n",
    "This step is crucial since it insures quality data is used during your analysis. <br> For Machine Learning models, it is essential for an algorithm to produce accurate and reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDsJcqR0QD-J"
   },
   "source": [
    "**Presentation of the use case** <br>\n",
    "For today's course, we will analyze a dataset with 6000 spotify tracks and 20 features. <br>\n",
    "The goal of the project will be to:\n",
    "1. Build a Machine Learning model that can **predict the genre of a song**\n",
    "2. Build a Machine Learning model that can **predict the popularity of a song**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ROzl_UkblFh"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you are running this notebook on google colab\n",
    "# !wget https://raw.githubusercontent.com/hi-paris/Hickathon5/refs/heads/main/courses/Feature_Engineering/spotify_tracks.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvQCqbHdim2Y"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a Pandas DataFrame\n",
    "dataset = pd.read_csv(\"spotify_tracks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EL02zFIapFdC"
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuxVwopMSsOn"
   },
   "source": [
    "Here is a description of each column in the dataset. <br>\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ------------ |\n",
    "| **track_id** | The Spotify ID for the track |\n",
    "| **artists** | The artists' names who performed the track |\n",
    "| **album_name** | The album name in which the track appears |\n",
    "| **track_name** | Name of the track |\n",
    "| **popularity** | The popularity of a track is a value between 0 and 100.\n",
    "| **duration_ms** | The track length in milliseconds |\n",
    "| **explicit** | Whether or not the track has explicit lyrics |\n",
    "| **danceability** | Danceability describes how suitable a track is for dancing |\n",
    "| **energy** | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity\n",
    "| **key** | The key the track is in. Integers map to pitches using standard Pitch Class notation |\n",
    "| **loudness** | The overall loudness of a track in decibels (dB) |\n",
    "| **mode** | Mode indicates the modality (major or minor) of a track (major=1, minor=0) |\n",
    "| **speechiness** | Speechiness detects the presence of spoken words in a track |\n",
    "| **acousticness** | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. |\n",
    "| **instrumentalness** | Predicts whether a track contains no vocals |\n",
    "| **liveness** | Detects the presence of an audience in the recording|\n",
    "| **valence** | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track |\n",
    "| **tempo** | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology |\n",
    "| **time_signature** | An estimated time signature. The time signature (meter) is a notational convention <br> to specify how many beats are in each measure. |\n",
    "| **track_genre** | The genre in which the track belongs |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPvuHZXdTFpQ"
   },
   "source": [
    "You can find more information on the original dataset [here](https://huggingface.co/datasets/maharshipandya/spotify-tracks-dataset). <br>\n",
    "**Note**: We will only use a subset of the original \"spotify-tracks-dataset\" in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqoHnaiLjah-"
   },
   "outputs": [],
   "source": [
    "# Get general information on the dataset (ncol, nrow, nbr missing values, dtypes)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMhJKmPZ3fpB"
   },
   "source": [
    "The `.info()` function allows you to check:\n",
    "- The number of \"non-null\" values (number of rows without a `NaN` value)\n",
    "- The data type (or `dtype` of each column)\n",
    "- The number of rows and the columns present in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_lyVSTu4pYF"
   },
   "source": [
    "**<u>Reminder</u>**: **Pandas object have specific dtypes** (data types)\n",
    "- `int64`/`float64`: Equivalent to `int`/`float` (numerical values)\n",
    "- `object`: Strings or mixed types\n",
    "- `bool`: Boolean values (`True` or `False`)\n",
    "- `datetime64[ns]`: Date and time information.\n",
    "- `timedelta[ns]`: Difference between two datetimes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry7EDXRL7g8-"
   },
   "source": [
    "**<u>Note</u>**: Date-related columns are often loaded as `object` types, instead of a datetime dtype. Converting these columns into `datetime64[ns]` using the `pd.to_datetime()` function is a good practice as it allows the use of `.dt` function for date manipulation.\n",
    "\n",
    "Learn more on how to manipulate timeseries data with pandas: https://pandas.pydata.org/docs/user_guide/timeseries.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZLJCnfIv7J3"
   },
   "source": [
    "## **1. Handle missing values**\n",
    "\n",
    "Real-word data often contains missing values. This can be due to technical failures, data entry issues, incompatible data formats or even intentional omissions (for privacy concerns). Examples include a lack of response to a survey or equipment failures when collecting data.\n",
    "\n",
    "Here is a list of strategies to handle missing values:\n",
    "- Replace NaN with the **mean** or **median value** (for continuous data)\n",
    "- Replace NaN with the **most frequent value** (for categorical data)\n",
    "- Create a **new category** for the missing values (for categorical data)\n",
    "- Use a **Machine Learning model** to predict the missing values (KNN, Random Forest,...)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhcH7XLSIiqo"
   },
   "outputs": [],
   "source": [
    "# Number of missing values for each variable\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tw4fE8ORLspa"
   },
   "source": [
    "**<u>Reminder</u>: Continuous vs Categorical data** <br>\n",
    "- **Categorical variables** are made of discrete values that fall into distinct categories or groups. <br>\n",
    "Here is how you can identify them in a dataset:\n",
    "  - They usually take a small number of unique values (but not always the case).\n",
    "  - They are often encoded as `object` types with pandas but they can also be listed as `int` types.\n",
    "- **Continuous variables** are numerical values that can take any value within a particular range.\n",
    "  - Examples: Price, Weight, Speed...\n",
    "  - Note: Not all numerical variable are continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1oa9EOeYJf1"
   },
   "outputs": [],
   "source": [
    "# Get the data types of the selected variables\n",
    "dataset[[\"album_name\",\"explicit\",\"energy\"]].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwJdDVn_B4Re"
   },
   "outputs": [],
   "source": [
    "# Get the number of unique values of the selected variables\n",
    "dataset[[\"album_name\",\"explicit\",\"energy\"]].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PueulJAJveTq"
   },
   "source": [
    "**Tips**:\n",
    "- A mean value should be used on variables with a \"normal\" distribution (no strong skewness, outliers).\n",
    "- A median value should be used when the distribution is skewed with many outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gmvs1M_g1ly9"
   },
   "outputs": [],
   "source": [
    "# Use sklearn's SimpleImputer function to replace mean (or median)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "dataset[\"energy\"] = mean_imputer.fit_transform(dataset[[\"energy\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zNFkboIim2Z"
   },
   "outputs": [],
   "source": [
    "# Use Pandas' fillna() function to replace with most frequent value\n",
    "most_frequent_value = dataset[\"explicit\"].value_counts().index[0]\n",
    "dataset[\"explicit\"] = dataset[\"explicit\"].fillna(most_frequent_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVnD6dI-JORt"
   },
   "source": [
    "You can also chose to delete the rows or columns that contain missing values. This can be a viable option in cases when the amount of missing values is small.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = https://phoenixnap.com/kb/wp-content/uploads/2021/06/drop-missing-values-visual-example.png width = \"500\" height = \"250\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBj6m4Z_10ZD"
   },
   "outputs": [],
   "source": [
    "# # Drop the remaining missing values (only in the explicit variable)\n",
    "# dataset.dropna(inplace=True, subset=\"explicit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD8kF5t8STK0"
   },
   "source": [
    "**Recommandation**: There is no \"one-size-fits-all\" solution to handle missing values. Evaluate the impact of different imputation strategies on your model and be careful to not introduce to much bias when using them. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QapjTw_jwCd2"
   },
   "source": [
    "## **2. Remove variables**\n",
    "\n",
    "**You don't need to include every variable of the dataset to the prediction model.**\n",
    "- Some variable are difficult to clean (because of a large number of missing values or missing values that are complicated to fill). An example in our dataset is `album_name`.\n",
    "\n",
    "- Other variables might not be useful to complete your prediction task. For example, `track_id` is a unique identifier for each song which doesn't hold much valuable information to predict a track's genre or popularity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_vKe40h1ya3"
   },
   "outputs": [],
   "source": [
    "# Drop duplicate rows and irrelevant features\n",
    "dataset.drop_duplicates(inplace=True)\n",
    "dataset.drop(columns=[\"track_id\",\"album_name\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7l7mrRswHCF"
   },
   "source": [
    "## **3. Clean text data**\n",
    "\n",
    "Handling text data in any Machine Learning project can be a difficult task.\n",
    "Valuable information might be present in a text variable that can't used by a model in its raw form.  \n",
    "\n",
    "To clean variables containing large amounts of text, you can try the following methods:\n",
    "- Lowercasing each word\n",
    "- Removing \"stopwords\" with little significance (examples: `a`, `an`, `the`, `and`, `but`, ...)\n",
    "- Spell checking and correction\n",
    "- Removing punctuation and special characters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cf-PPacWiTK7"
   },
   "source": [
    "**Text cleaning tools**: <br>\n",
    "In Python, cleaning text data often involves using regex patterns and NLP libraries such as `nltk` and `spacy`. <br>\n",
    "\n",
    "In this course we won't cover NLP specific preprocessing techniques such as tokenization and stemming. <br> If you want to learn more, you can read the following [article](https://medium.com/mlearning-ai/nlp-tokenization-stemming-lemmatization-and-part-of-speech-tagging-9088ac068768)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtQ2xbzmTgi6"
   },
   "outputs": [],
   "source": [
    "# Regex pattern to remove punctuation\n",
    "pattern = r'[\\[\\]()\\-:;\",/\\.\\.\\.‚Äò\\'‚Äô?!‚Äú&]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "320-bYP3Trpo"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you are working on Google colab or any other IDE\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words(fileids=('english', 'spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiXpiCEH5Z0N"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you are working with JupyterLite\n",
    "# with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as text:\n",
    "#     stopwords = text.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkEZuokbim2a"
   },
   "outputs": [],
   "source": [
    "# Add more words to stopwords\n",
    "stopwords = [word.replace(\"\\n\",\"\") for word in stopwords]\n",
    "stopwords.extend([\"love\",\"remastered\",\"remaster\",\"feat\",\"remix\",\"version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_pQ7Ld4VQ4A"
   },
   "outputs": [],
   "source": [
    "clean_tracks = []\n",
    "for track in dataset[\"track_name\"].to_list():\n",
    "  # remove punctuation, numbers\n",
    "  track = track.lower()\n",
    "  track = re.sub(pattern,\"\",track).replace(\"  \",\" \")\n",
    "\n",
    "  # remove stopwords\n",
    "  clean_text = [text for text in track.split(\" \") if text not in stopwords]\n",
    "  clean_text = \" \".join(clean_text)\n",
    "  clean_tracks.append(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEaykkkSUE9I"
   },
   "outputs": [],
   "source": [
    "dataset[\"track_name\"] = clean_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMvi1z4GZ4FC"
   },
   "source": [
    "We will also clean the `artists` variable by only selecting the main artist (the first artist listed). <br>\n",
    "This will help simplify the data preprocessing step we will perform latter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOra6He6RoD-"
   },
   "outputs": [],
   "source": [
    "# Create a function that selects the first artist\n",
    "def clean_artists(x):\n",
    "  return x.split(\";\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bVtaVTx4L-d"
   },
   "outputs": [],
   "source": [
    "dataset[\"artists\"] = dataset[\"artists\"].apply(clean_artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdLmzyvYmC0U"
   },
   "source": [
    "Here are other examples of data cleaning:\n",
    "- **Fixing incorrect data types** (example: Numerical variables that have an `object` dtype, instead of `int64`/`float64`)\n",
    "- **Replacing incorrect values/misspellings** (example: Missing values written as 'NaN' instead of a true `NaN` value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kegBicJsJKeK"
   },
   "source": [
    "# **II. Data Exploration üîç**\n",
    "Data Exploration refers to the process of performing initial investigations on the data, with the help of summary statistics and graphical representations. It is mainly used for these three reasons:\n",
    "- **Discover patterns and relationships**: Detect bias in the data, find relevant variables for your analysis\n",
    "- **Detect anomalies and outliers**: Identify extreme values and potentially remove them\n",
    "- **Apply statistical tests**: Test correlation, non-linearity, independance...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEYTT8s3im2b"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you are working with JupyterLight\n",
    "# import piplite\n",
    "# await piplite.install('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMhFGGZYYV37"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Apc9NLQZb6Ia"
   },
   "source": [
    "Here is a reminder of popular data visualization libraries in Python:\n",
    "- `matplotlib`: Foundational and highly customizable plotting library\n",
    "- `seaborn`: High-level plotting library built on top of Matplotlib\n",
    "- `plotly`: Interactive and web-based plotting library\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = https://coding-blocks.github.io/DS-NOTES/_images/matplotlib1.png width = \"600\" height = \"300\" >\n",
    "\n",
    "Some popular plot types include: scatter plots, histograms and bar plots. <br>\n",
    "For more examples of plots, you can visit the Python Graph Gallery [here](https://python-graph-gallery.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPjUCpMqE4Fk"
   },
   "source": [
    "## **1. Univariate Data Analysis**\n",
    "\n",
    "Univariate Data Analysis involves **analyzing and describing the characteristics of variables individually**. <br>\n",
    "\n",
    "It includes studying a variable's distribution and range in the case of continuous data. <br> For categorical data, it can mean analyzing the frequency of each possible values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KP7Ns5nrHC8"
   },
   "source": [
    "**Frequency plot** <br>\n",
    "Categorical variables tend to have a small number of unique values. <br> To study these kinds of variables, we will plot the frequency of each category in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pltmbOzdt1UP"
   },
   "outputs": [],
   "source": [
    "# Select categorical variables\n",
    "categorical_var = dataset.select_dtypes(include=[\"object\"]).columns.to_list()\n",
    "categorical_var.extend([\"key\",\"mode\",\"time_signature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PklWMtGQvLcc"
   },
   "outputs": [],
   "source": [
    "# Number of unique values per variable\n",
    "dataset[categorical_var].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo6WWjMz7Dme"
   },
   "outputs": [],
   "source": [
    "# Select categorical variables with small number unique values\n",
    "df_barplot = dataset[[\"explicit\",\"track_genre\",\"mode\",\"time_signature\",\"key\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIvJrQW0ukeM"
   },
   "outputs": [],
   "source": [
    "# Create barplot with frequency for each variable\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "for c,var in enumerate(df_barplot.columns):\n",
    "  # compute frequency of each unique value\n",
    "  df = df_barplot[var].value_counts(normalize=True).to_frame(\"frequency\").reset_index(names=var)\n",
    "  df[\"frequency\"] = df[\"frequency\"]*100\n",
    "\n",
    "  # plot the barplot\n",
    "  plt.subplot(3,2,c+1)\n",
    "  sns.barplot(data=df, x=var, y=\"frequency\")\n",
    "  plt.title(str(var))\n",
    "  plt.xlabel(\"\")\n",
    "  plt.ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5op_xsI4O6P"
   },
   "source": [
    "**Histogram/Distribution** <br>\n",
    "Histograms should only be used for continuous data. <br> They allow you identify whether the distribution of a variable is symmetric (normal distribution), skewed or bimodal.\n",
    "\n",
    "\n",
    "<img src = https://chartio.com/assets/db1384/tutorials/charts/histograms/8f583a5fe6872609b95c00a4c75a30b9bf3f0ee012ca6590563e3510ffd14841/histogram-example-2.png width = \"500\" height = \"300\" >\n",
    "\n",
    "They can give you an indication on how to fill a missing value or what ML model to select (since some require a normal distribution). <br> It is also a great way to detect bias in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPTfFwOY7vOt"
   },
   "outputs": [],
   "source": [
    " # Select continuous variables\n",
    "continuous_var = dataset.drop(columns=categorical_var).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2tW-AnrpAZo"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,12))\n",
    "\n",
    "for c,var in enumerate(continuous_var):\n",
    "  plt.subplot(6,2,c+1)\n",
    "  sns.kdeplot(data=dataset[continuous_var], x=var, fill=var)\n",
    "  plt.title(str(var))\n",
    "  plt.xlabel(\"\")\n",
    "  plt.ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCADyizKAKQx"
   },
   "source": [
    "**Detect outliers** <br>\n",
    "Outliers are data points that significantly deviate from the majority of the data. <br> Keeping these \"extreme\" values can lead to suboptimal performances and can hinder a model's ability to generalize.\n",
    "\n",
    "Outliers are usually detected using **summary statistics** such as mean, median, first and third quartile.\n",
    "\n",
    "Outliers can also be detected using statistical plots such as **boxplots** that show where most of the data is concentrated (in the interquartile range) as well as which points are \"extreme\" (lower than the minimum or larger than the maximum).\n",
    "\n",
    "<img src = https://miro.medium.com/max/9000/1*2c21SkzJMf3frPXPAR_gZA.png width = \"600\" height = \"350\" >\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8MxIlHtpzfX"
   },
   "outputs": [],
   "source": [
    "# Use .describe() to compute the summary statistics\n",
    "dataset[continuous_var].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ_H0nxs1-XQ"
   },
   "source": [
    "We will create a boxplot for variables whose values are not known to be in a defined range, such as 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjixZRqj2NXV"
   },
   "outputs": [],
   "source": [
    "# Create boxplots for continuous variables without a defined range\n",
    "continuous_var_other = [\"popularity\",\"duration_ms\",\"loudness\",\"tempo\"]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for c,var in enumerate(continuous_var_other):\n",
    "  plt.subplot(2,2,c+1)\n",
    "  ax = sns.boxplot(data=dataset[continuous_var_other], y=var)\n",
    "  ax.ticklabel_format(style='plain', axis='y')\n",
    "  plt.title(str(var))\n",
    "  plt.xlabel(\"\")\n",
    "  plt.ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3_MS9nP7IUj"
   },
   "source": [
    "The `duration_ms` variable has a few outliers with very large values (much higher than its regular distribution). <br>To identify whether these values are possible even if extreme, domain knowledge can be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuCSaVtx-Fkk"
   },
   "outputs": [],
   "source": [
    "dataset[\"duration_ms\"].max()/60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7niOnB7_ryMO"
   },
   "source": [
    "The track with the highest duration is 44min, which is long but not impossible. <br> These extreme values are not necessary outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDHle5KHAiQ4"
   },
   "source": [
    "**In many cases, detecting outliers also requires domain knowledge.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFhgtZqQE_W-"
   },
   "source": [
    "## **2. Multivariate Data Analysis**\n",
    "\n",
    "Multivariate Data Analysis is used to analyze the **relationship between multiple variables** in a dataset. <br>\n",
    "\n",
    "It can involve studying the relationship between the target variable and the features that will be used to predict it. It can also be used to compare different feature variables with correlation for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SSVdMioM_q_"
   },
   "source": [
    "<img src = https://editor.analyticsvidhya.com/uploads/78011Picture3.png width = \"600\" height = \"350\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TomKOme8QTk"
   },
   "source": [
    "**Categorical and Continuous** <br>\n",
    "In this example, we plotted the distribution of three variables (energy, acousticness and danceability) based on the genre. <br>\n",
    "Since one of the variables is continuous, we can use a histogram.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eINJHijMkDlK"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, sharex=True, figsize=(8,8))\n",
    "\n",
    "for ax, var in zip(axes, [\"energy\", \"acousticness\", \"danceability\"]):\n",
    "  sns.kdeplot(data=dataset, x=var, hue=\"track_genre\", fill=\"track_genre\",ax=ax);\n",
    "  ax.set_title(var)\n",
    "  ax.set_xlabel(\"\")\n",
    "  ax.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ou8Zx4evt2l"
   },
   "source": [
    "**Continuous and Continuous** <br>\n",
    "To study multiple continuous variables, we computed the correlation coefficient between each one. <br>\n",
    "\n",
    "Correlation is often used to identify redundant variables that can be removed from the analysis. <br> A high correlation (in absolute value) is usually above 0.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iixjlXnuvzsl"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(dataset[continuous_var].corr().abs(),annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mxY6KbRr6CO"
   },
   "source": [
    "**Categorical and Categorical** <br>\n",
    "*Stacked bar plots* are often used for multiple categorical variables. <br>\n",
    "It can show how the data is distributed based on two categorical variables, instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcgE4L1SQehj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# create dataframe to count track_genre/mode\n",
    "df_genre_explicit = dataset[[\"track_genre\",\"explicit\"]].value_counts().to_frame(\"count\").reset_index()\n",
    "\n",
    "# add row for classifical genre with track_genre = True (frequency=0)\n",
    "classical_explicit_True = {\"track_genre\":\"classical\",\"explicit\":True, \"count\":0}\n",
    "df_genre_explicit = pd.concat([df_genre_explicit, pd.DataFrame([classical_explicit_True])], ignore_index=True)\n",
    "\n",
    "# create barplots\n",
    "bar1 = sns.barplot(data=dataset[\"track_genre\"].value_counts().to_frame(\"count\").reset_index(names=\"track_genre\"), x=\"track_genre\", y=\"count\", color='lightblue');\n",
    "bar2 = sns.barplot(data=df_genre_explicit.loc[df_genre_explicit[\"explicit\"]==True], x=\"track_genre\", y=\"count\", color='darkblue',);\n",
    "\n",
    "# add legends\n",
    "bottom_bar = mpatches.Patch(color='darkblue', label='explicit = True')\n",
    "top_bar = mpatches.Patch(color='lightblue', label='explicit = False')\n",
    "plt.legend(handles=[top_bar, bottom_bar])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aq4mNTg23Jd"
   },
   "source": [
    "You can also create multiple *frequency (bar) plots* for each category, instead of a stacked plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwcoTSgpQJNX"
   },
   "outputs": [],
   "source": [
    "# Top 3 artists per genre\n",
    "top_artists_per_genre = dict()\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for c, genre in enumerate(dataset[\"track_genre\"].unique()):\n",
    "  df = dataset.loc[dataset[\"track_genre\"]==genre,\"artists\"].value_counts(normalize=True).to_frame(\"frequency\").reset_index(names=\"artists\")\n",
    "  df = df.sort_values(by=[\"frequency\"],ascending=False)\n",
    "  df[\"frequency\"] = (100*df[\"frequency\"]).round(2)\n",
    "  top_artists_per_genre[genre] = df.head(1)[\"artists\"].to_list()[0]\n",
    "\n",
    "  plt.subplot(3,2,c+1)\n",
    "  sns.barplot(data=df.head(5), x=\"frequency\", y=\"artists\", hue=\"artists\")\n",
    "  plt.title(genre)\n",
    "  plt.xlabel(\"\")\n",
    "  plt.ylabel(\"\")\n",
    "  plt.legend(\"\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MVExKbb3L6x"
   },
   "source": [
    "<u>Note</u>: When a categorical variable has large number of unique values, you can try to group some of these categories together. <br> This could be based on their frequency in the data or information you have on these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq7P7Twn3sYX"
   },
   "outputs": [],
   "source": [
    "top_artists = dataset[\"artists\"].value_counts(normalize=True).head(6).index.to_list()\n",
    "dataset[\"artists\"] = dataset[\"artists\"].apply(lambda x: x if x in top_artists else \"Other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhyXnq_93Vz9"
   },
   "source": [
    "For variables with a lot of text, you can use *word clouds* to represent frequent words. <br>\n",
    "We will use this type of visualization to represent top words in tracks based on the genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1IYZicx1Co9"
   },
   "outputs": [],
   "source": [
    "# Create a dict with a single string for each genre\n",
    "dict_track_genre = dict()\n",
    "for genre in dataset[\"track_genre\"].unique():\n",
    "  list_tracks = dataset.loc[dataset[\"track_genre\"]==genre,\"track_name\"].to_list()\n",
    "  dict_track_genre[genre] = \" \".join(list_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As2BunqP08Jp"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you are working on Google colab or any other IDE\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "top_words_track = []\n",
    "f, axes = plt.subplots(3,2,figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for genre,ax in zip(dataset[\"track_genre\"].unique(),axes):\n",
    "  wordcloud = WordCloud(width = 800, height = 500,\n",
    "                        background_color ='white',\n",
    "                        stopwords = stopwords,\n",
    "                        min_font_size = 25).generate(dict_track_genre[genre])\n",
    "\n",
    "  top_words_track.append(list(wordcloud.words_.keys())[0])\n",
    "\n",
    "  ax.imshow(wordcloud)\n",
    "  ax.set_title(genre)\n",
    "  ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgwsMhUIim2h"
   },
   "outputs": [],
   "source": [
    "# Run this cell if you are working on JupyterLite.\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# # Image 1\n",
    "# plt.figure(figsize=(18,16))\n",
    "# plt.subplot(2,1,1)\n",
    "# img = mpimg.imread('wordcloud1.PNG')\n",
    "# imgplot = plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "\n",
    "# # Image 2\n",
    "# plt.figure(figsize=(18,8))\n",
    "# plt.subplot(2,1,2)\n",
    "# img = mpimg.imread('wordcloud2.PNG')\n",
    "# imgplot = plt.imshow(img)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sW4W32DUYeMe"
   },
   "outputs": [],
   "source": [
    "dataset.drop(columns=[\"track_name\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itzGgQilRhH-"
   },
   "source": [
    "# **III. Feature Engineering** ‚õèÔ∏è (Data Preprocessing)\n",
    "\n",
    "An essential step before creating a Machine Learning model is Data Preprocessing. <br> It consists of transforming features of the dataset into a proper format for a model. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmpqn_MQygDd"
   },
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=[\"track_genre\"]) # features\n",
    "y = dataset[\"track_genre\"] # target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-blo4xg7xPW"
   },
   "source": [
    "**<font size=4> <u>Train, test split</u></font>** <br>\n",
    "To be able to test the performance of a model on unseen data, the original dataset is split into a **training set** and a **test set**. <br> In most cases, a new set of observations cannot be used to test the model.\n",
    "- The training set is used to train the model and learn parameters (70%-80% of the data)\n",
    "- The test set is used to test the model on unseen data during training (20%-30% of the data)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src = https://ugc.futurelearn.com/uploads/assets/05/8a/058ad514-cb51-4107-855e-23aeace3d0e3.png width = \"600\" height = \"300\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qq5txSCHfUtj"
   },
   "source": [
    "**Tips**:\n",
    "- The choice in the size of the test split mostly depends on the size of the original data. Small datasets usually require the test set to be small (20%), whereas large datasets can include a larger test set.\n",
    "\n",
    "- If the **target variable is unbalanced** (skewed distribution of classes), you can use the `stratify=y` parameter. This will insure each class is well represented in both splits.\n",
    "\n",
    "- It is recommended to apply data preprocessing on each set seperatly, as to avoid **Data Leakage**. The test set shouldn't contain information from the training set, as to simulate unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AG0XiHDBPHT"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkG32g8Axkw-"
   },
   "source": [
    "## **1. Categorical encoding**\n",
    "Categorical encoding refers to the process of **converting categorical data into numerical format**, so that it can be used as input for algorithms to process. Most Machine Learning algorithms work with numerical data, not text so this step is essential.\n",
    "\n",
    "Many types of encoders can be used on categorical variables. Here are important questions you should ask before selecting one.\n",
    "- *What are the catergorical variables in the dataset ?* (excluding the target)\n",
    "- *What are the number of unique values of the variable ?* (low or high cardinality)\n",
    "- *Is the variable ordinal or nominal ?* (ordered or unordered categories)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xz4r869jkmmc"
   },
   "source": [
    "<img src = https://miro.medium.com/v2/resize:fit:756/1*MAr4rWj6zw0Rdo01ecZu1A.png width = \"650\" height = \"400\" >\n",
    "\n",
    "<br>\n",
    "\n",
    "Here are a few common catgeorical encoding methods:\n",
    "- Low cardinality and nominal: **One-hot encoding**\n",
    "- High cardinality and nominal: **Frequency Encoding, Target Encoding, Hash Encoding**\n",
    "- Ordinal variables: **Label encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGM4hmQTBy_X"
   },
   "source": [
    "**What are the categorical features in the dataset ?** <br>\n",
    "Our dataset has five categorical features, three of them are integers, one is a string and one is a boolean. <br>\n",
    "`mode` and `time_signature` don't need to be encoded as they already have the proper format.\n",
    "\n",
    "<br>\n",
    "\n",
    "| Variable | Description | dtype |\n",
    "|----------| ----------- | ----- |\n",
    "| artists | Nominal, High cardinality (1925 unique values) | string\n",
    "| explicit | Nominal, Low cardinality (True/False) | boolean\n",
    "| key | Nominal, Low cardinality (11 unique values) | int\n",
    "| mode | Nominal, Low cardinality (0/1) | int\n",
    "| time_signature | Ordinal, Low cardinality (4 unique values) | int\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSvrbAbXCTLh"
   },
   "source": [
    "<font size=4> <b><u>OneHotEncoding</u> </b> </font> <br>\n",
    "OneHotEncoding consists of creating a binary variable for each category. <br>\n",
    "It is the prefered method of encoding when a categorical variable is nominal.\n",
    "\n",
    "\n",
    "<img src = https://miro.medium.com/v2/resize:fit:1200/1*ggtP4a5YaRx6l09KQaYOnw.png width = \"600\" height = \"200\">\n",
    "\n",
    "OneHotEncoding shouldn't be used on variables with a high cardinality. <br>\n",
    "If used, it can add a huge amount of variables which can overcomplexify your model and lead to slow model training.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PF1-Fupywena"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-HYrbokOnsU"
   },
   "outputs": [],
   "source": [
    "# Select the variables to OneHotEncode in the train and test set\n",
    "var_onehot = [\"explicit\",\"key\",\"artists\"]\n",
    "onehot_df_train = X_train[var_onehot]\n",
    "onehot_df_test = X_test[var_onehot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cA4p3o8FmHG"
   },
   "outputs": [],
   "source": [
    "# Fit the OneHotEncoder to the training set\n",
    "onehot_enc = OneHotEncoder(sparse_output=False)\n",
    "onehot_enc.fit(onehot_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SWZoXBaarcYj"
   },
   "outputs": [],
   "source": [
    "# Get the new feature names created\n",
    "onehot_features = onehot_enc.get_feature_names_out()\n",
    "\n",
    "onehot_df_train_t = pd.DataFrame(onehot_enc.transform(onehot_df_train).astype(\"int\"), columns=onehot_features)\n",
    "onehot_df_test_t = pd.DataFrame(onehot_enc.transform(onehot_df_test).astype(\"int\"), columns=onehot_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWuQ_rigkYe8"
   },
   "outputs": [],
   "source": [
    "onehot_df_train_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8F1nBbsO1E9"
   },
   "outputs": [],
   "source": [
    "# Recreate the train and test sets with the newly encoded variables\n",
    "X_train = pd.concat([X_train.drop(columns=var_onehot).reset_index(drop=True), onehot_df_train_t],axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=var_onehot).reset_index(drop=True), onehot_df_test_t],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnUZjUum_DEt"
   },
   "source": [
    "<font size=4> <b><u>Label Encoding</u> </b> </font> <br>\n",
    "For ordinal variables, you can try Label Encoding which will assign each category to a numerical value. <br> It isn't recommended to use this encoder on nominal variables as it might lead the model to misinterpret your variable.\n",
    "\n",
    "<img src = https://www.statology.org/wp-content/uploads/2022/08/labelencode2-1.jpg width = \"400\" height = \"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oUizy_MbsN-"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ilf6Q0SVDT2I"
   },
   "source": [
    "<font size=4> <b><u>Other methods:</u></b> </font> <br>\n",
    "Other categorical encoding techniques can be used on features with a high cardinality. <br>\n",
    "\n",
    "**a) Frequency encoding**: <br>\n",
    "This method replaces each category in a variable by its frequency in the data.\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src = https://www.elastic.co/guide/en/machine-learning/current/images/frequency-encoding.jpg width = \"600\" height = \"250\" >\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbqY_mg7vLE7"
   },
   "source": [
    "**b) Target encoding**: <br>\n",
    "This method replaces each category by their average target value/frequency. <br> If the target variable is multi-class, a variable shoud be created for each class to predict. <br>\n",
    "\n",
    "<u>Warning </u>: Target encoding can sometimes lead to **Target Leakage**. <br> This occurs when the model is built, or trained, with information that will not be available in unseen data, such as information about the target variable. This could lead to falsely high results in the train set.\n",
    "\n",
    "<br>\n",
    "\n",
    "<!-- <img src = https://miro.medium.com/v2/resize:fit:419/1*W77md1OC9HSuAFy9b0LEIw.png width = \"300\" height = \"400\" > -->\n",
    "\n",
    "<img src = https://miro.medium.com/v2/resize:fit:1016/1*5F7LDKbf1qRw9yvbJ2WJyw.png width = \"450\" height = \"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YvlF-moEi6d"
   },
   "source": [
    "**c) Hash encoding** <br>\n",
    "Hash Encoding encodes categorical data into numerical value using a **\"hashing function\"**. The hashing function **maps each category to a pre-determined and fixed number of numerical columns**, instead of creating a column for each category.\n",
    "\n",
    "This method reduces high cardinality if you set the number of numerical columns much lower than the number of categories.\n",
    "\n",
    "<u>Warning</u>:\n",
    "- This method can lead to <u>information loss</u> and reduce interpretability since we transform the data into fewer features\n",
    "- Since a high number of categorical values are represented into a smaller number of features, different categorical values could be represented by the same Hash values. This is called a <u>collision</U>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g9WTElXxsLZ"
   },
   "source": [
    "## **2. Feature scaling**\n",
    "\n",
    "Feature scaling consists of transforming all of the (continuous) variables in a dataset to a **similar scale**. <br>\n",
    "This ensures that all features contribute equally to the model and avoids the dominance of features with larger values. It can be important for models that use euclidean distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL6pfpS81e6W"
   },
   "outputs": [],
   "source": [
    "# Select continuous variables\n",
    "var_scaling = X_train.select_dtypes(include=[\"float64\"]).columns.to_list()\n",
    "var_scaling.extend([\"popularity\",\"duration_ms\",'time_signature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hA5iDultDX_M"
   },
   "outputs": [],
   "source": [
    "dataset[var_scaling].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8urJgJp_qTw"
   },
   "source": [
    "Here are two common methods for feature scaling:\n",
    "- **Normalization**  (`MinMaxScaler`): Scale features to a given range, usually between 0 and 1. <br>\n",
    "Normalization is recommended for variables without a Gaussian distribution or with a small standard deviation. <br>\n",
    "It isn't recommnded for variables with outliers.\n",
    "\n",
    "- **Standardization**  (`StandardScaler`): Scale features by removing the mean and scaling to unit variance. <br>\n",
    "Standardization can help reduce the presence of outliers but doesn't guarantee balanced feature scales in the presence of outliers. <br>\n",
    "Outliers have an influence on the empirical mean and standard deviation computed for Standardization.\n",
    "\n",
    "<img src = https://miro.medium.com/v2/resize:fit:744/1*HW7-kYjj6RKwrO-5WTLkDA.png width = \"600\" height = \"300\" >\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_WYPHRq11Eg"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wALElLcT2-2m"
   },
   "outputs": [],
   "source": [
    "scaler_std = StandardScaler() # Standardization\n",
    "#scaler_minmax = MinMaxScaler() # Normalization\n",
    "\n",
    "scaler_std.fit(X_train[var_scaling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGtJNZ9T3GKZ"
   },
   "outputs": [],
   "source": [
    "X_train[var_scaling] = scaler_std.transform(X_train[var_scaling])\n",
    "X_test[var_scaling] = scaler_std.transform(X_test[var_scaling])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfJE_1M9rPF2"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8rUsemIim2k"
   },
   "source": [
    "For variables with a **very skewed distribution**, it is recommended to use other types of scalers:\n",
    "- `RobustScaler`: Scale features using statistics that are robust to outliers (median and interquartile distance). <br>\n",
    "The scaling is thus not influenced by a small number of very large marginal outliers and transformed variables tend to have a similar range.\n",
    "\n",
    "- `PowerScaler`: Apply a power transform (yeo-johnson or box-cox) to make features more Gaussian-like. <br> These non-linear transformation can reduce the scale of outliers in a variable.\n",
    "\n",
    "To learn about the impact of different scalers on variables, you can read this [page](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQfga1Tvim2k"
   },
   "source": [
    "<font size=4> <b>Bonus: <u>Feature selection</u></b> </font> <br>\n",
    "Another popular preprocessing techniques is **Feature selection**. <br>\n",
    "Feature selection is the process of selecting a subset of relevant features for your model.\n",
    "\n",
    "Scikit-learn has many functions to perform Feature selection (`VarianceThreshold`, `SelectKBest`, `RFE`...)  <br>\n",
    "You can find more information about these methods [here](https://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "<img src = https://blog.knoldus.com/wp-content/uploads/2022/02/feature-selection-techniques-in-machine-learning2.png width = \"600\" height = \"400\" >\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92AIG8k1lS5h"
   },
   "source": [
    "## **3. Scikit-learn pipeline**\n",
    "\n",
    "Pipelines can be useful to test different strategies to replace missing values as well as multiple data pre-processing methods. <br>\n",
    "It also allows you to transform the training and test set seperatly and limit the risks of Data Leakage.\n",
    "\n",
    "Scikit-learn has two functions to create preprocessing pipelines:\n",
    "- `Pipeline`: Apply multiple transformations to the same columns\n",
    "- `ColumnTransformer`: Transform each column set separately before combining them later.\n",
    "\n",
    "<img src = https://miro.medium.com/v2/resize:fit:1200/1*LdwXVtec9-Byt-lOO7Csyg.png width = \"600\" height = \"300\" >\n",
    "\n",
    "Any scikit-learn transformer (encoder, scaler,...) can be used in a pre-processing pipeline. <br>\n",
    "You can also add a missing value imputer (for example `SimpleImputer`) and Machine Learning models to a pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayv7eMPfuFK_"
   },
   "source": [
    "Let's build a pipeline to predict the popularity variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ploMAH4BCoc_"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xbNMS86XGXb"
   },
   "outputs": [],
   "source": [
    "# Train, test split for predicting \"popularity\"\n",
    "X_ = dataset.drop(columns=[\"popularity\"]) # features\n",
    "y_ = dataset[\"popularity\"] # target\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6xvhZVtXmAO"
   },
   "outputs": [],
   "source": [
    "# columns for categorical pipeline\n",
    "categorical_pipeline = X_.select_dtypes(include=[\"object\"]).columns.to_list()\n",
    "categorical_pipeline.append(\"key\")\n",
    "\n",
    "# columns for continuous pipeline\n",
    "continuous_pipeline = X_.drop(columns=categorical_pipeline + [\"mode\", \"time_signature\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shwE2RVfBD0s"
   },
   "outputs": [],
   "source": [
    "# Build the pipeline then fit it to training data\n",
    "pipe1 = ColumnTransformer(\n",
    "    [(\"categorical\", OneHotEncoder(sparse_output=False), categorical_pipeline),\n",
    "     (\"continuous\", StandardScaler(), continuous_pipeline)],\n",
    "    sparse_threshold=0)\n",
    "\n",
    "pipe1.fit(X_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQnWXyUjbX7j"
   },
   "outputs": [],
   "source": [
    "# Transform training and test set with pipeline\n",
    "X_train_1 = pipe1.transform(X_train_)\n",
    "X_test_1 = pipe1.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSm9hC5WeSwN"
   },
   "outputs": [],
   "source": [
    "# Get new feature names\n",
    "new_columns = [column.split(\"__\")[1] for column in pipe1.get_feature_names_out()]\n",
    "\n",
    "# Rebuild a dataframe with new features\n",
    "X_train_1 = pd.DataFrame(X_train_1, columns=new_columns)\n",
    "X_test_1 = pd.DataFrame(X_test_1, columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqTid_0panIO"
   },
   "source": [
    "**Tips**: If you need to apply more than one pre-processing step to a set of columns, you can use the `Pipeline` function to add these steps to `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7ud7rVdoOWR"
   },
   "source": [
    "<font size='5'>**The course ends here, thank you for listening !** </font><br>\n",
    "\n",
    "**See you November 14 at 6:30PM for the Hi! PARIS' Career Fair at T√©lecom Paris.**\n",
    "- The next course on **Pitch presentation** will be November 19th. <br>\n",
    "- See you November 21 for the next Data Science course on **Machine Learning**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_hi5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
