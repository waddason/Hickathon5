{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hed1pJyhB1MG"
   },
   "source": [
    "**<center><font color='#023F7C' size=\"6.5\">Course 3: Machine Learning </font>** <br>\n",
    "<font color=#023F7C size=4>**Hi!ckathon #5 - AI & Sustainability**</font> <br>\n",
    "<font color=#023F7C size=2> 6:15PM-8:00PM </font> <br>\n",
    "\n",
    "<img src = https://www.hi-paris.fr/wp-content/uploads/2020/09/logo-hi-paris-retina.png width = \"300\" height = \"200\" >\n",
    "\n",
    "</center>\n",
    "\n",
    "<font color=\"#023F7C\">**Authors**:</font> \n",
    "- **Hadrien Mariaccia**, Research Machine Learning Engineer @ Hi! PARIS <br>\n",
    "-  **Ga√´tan Brison**, Research Machine Learning Engineer Manager @ Hi! PARIS <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grLuCr2DDEh2"
   },
   "source": [
    "‚û°Ô∏è https://github.com/hi-paris ‚≠êÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut0tatC7B1MI"
   },
   "source": [
    "## Machine Learning : a short definition\n",
    "\n",
    "<h5>\n",
    "Machine Learning is a branch of AI that empowers computers to improve task performance by learning from data using algorithms and statistical models, <br> enabling them to make predictions or decisions without explicit programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZmv6Q7wB1MI"
   },
   "source": [
    "## **Machine Learning, problem typology overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sGq3nDFB1MI"
   },
   "source": [
    "<center>\n",
    "<img src = https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9G8g2xvaqGFJMPPGqLIlrQ.jpeg width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUuXz4u-B1MI"
   },
   "source": [
    "A good methodology to solve any problem is to relevently classify what is the type of the problem we want to solve.\n",
    "\n",
    "In Machine Learning, there's different class of models, each to be applied in specific use cases :\n",
    "\n",
    "- **Classical Machine Learning**:\n",
    "\n",
    "    - **Supervised Learning**: Trains a model on labeled data to learn the mapping between input and output, aiming for accurate predictions on new data.\n",
    "\n",
    "\n",
    "    - **Unsupervised Learning**: Involves algorithms analyzing unlabeled data to discover patterns or relationships, with common tasks including clustering and dimensionality reduction.\n",
    "\n",
    "\n",
    "- **Ensemble Learning**: Involves combining predictions from multiple models to enhance overall performance, leveraging the strengths and diversity of individual models.\n",
    "\n",
    "- **Reinforcement Learning**: Focuses on an agent learning decision-making through interactions with an environment. The agent receives feedback in the form of rewards or penalties to learn optimal strategies over time.\n",
    "\n",
    "\n",
    "- **Deep Learning**: A subfield of machine learning that concentrates on neural networks with multiple layers, known as deep neural networks, enabling automatic learning of complex hierarchical representations.\n",
    "\n",
    "<h4>Today's session is dedicated to <strong>Supervised Learning </strong> and <strong>Ensemble Learning </strong>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VVXyFtmB1MI"
   },
   "source": [
    "## **Today's Agenda** üó£Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmddG-ErB1MJ"
   },
   "source": [
    "#### **Presentation of the use case** <br>\n",
    "\n",
    "For today's course, we will use the dataset of **6000 spotify tracks cleaned and preprocessed** during the first course on **Feature Engineering**<br>\n",
    "\n",
    "The dataset is already split in a train / test part.\n",
    "\n",
    "Our use case is a **supervised learning** use case as we have labelled data\n",
    "\n",
    "#### **Agenda** <br>\n",
    "\n",
    "The goal of today's session will be to:\n",
    "\n",
    "1. Present **classification** models applied to the **prediction of the genre of a track** (6 different genres)\n",
    "2. Present **regression** models applied to the **prediction of the popularity of a track**\n",
    "3. Give some bibliography resources about **Time Series**\n",
    "\n",
    "#### **What we won't do** <br>\n",
    "\n",
    "1. Cover deep mathematical aspects of every algorithm\n",
    "2. Cover every classification and regression models\n",
    "3. Cover time series topic deeply\n",
    "4. Cover explainability methods and tools as it's the purpose of a further session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9Tt8OZSB1MJ"
   },
   "source": [
    "## **Classification algorithms to predict the genre of a track** üè∑Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48J0gLGzB1MJ"
   },
   "source": [
    "Our first use case is a **multi-class classification use case** as there's 6 different track genres. <br>\n",
    "\n",
    "<center>\n",
    "<img src = https://media.geeksforgeeks.org/wp-content/uploads/20200702103829/classification1.png width=500>\n",
    "</center>\n",
    "\n",
    "Almost every classification algorithm can be used for both binary and multi-class classification, but their performance are not the same depending on if it's a binary or multi-class classification.\n",
    "\n",
    "As this course has the ambition to cover the maximum number of machine learning concepts, and to do it by trying to give both key pratical and theoritical takeaways in the minimum amount of time : <br>\n",
    "\n",
    "**We won't cover every existing models**.\n",
    "\n",
    "\n",
    "In this section we will use and lightly deep dive into the following algorithms / models :\n",
    "\n",
    "1. K-Neirest Neighbors (KNN)\n",
    "2. Logistic Regression\n",
    "3. Decision Tree\n",
    "4. Random Forest\n",
    "5. XGBoost\n",
    "\n",
    "For each algorithm, we will use the following methodology :\n",
    "\n",
    "1. Slightly deep dive into the theory and show an explicit visualisation of how it works\n",
    "2. Discuss some of the hyperparameters of the algorithm\n",
    "3. Execute it using scikit-learn\n",
    "4. Show the accuracy of the algorithm on the test set\n",
    "\n",
    "Then, we will compare and evaluate these models using different **evaluation metrics** :\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix\n",
    "3. Cross-Validation\n",
    "4. Precision, Recall, F1 Score\n",
    "\n",
    "Finally, we will introduce **GridSearch** algorithm for **hyperpameter tuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcF6R5wSB1MJ"
   },
   "source": [
    "### Dataset description reminder\n",
    "\n",
    "As a reminder, here's the description of each column of the dataset. <br>\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ------------ |\n",
    "| **artists** | **The artists' names who performed the track, this column is one hot encoded <br> into 7 columns, one for each 6 top artists and one for an other artists column** |\n",
    "| **popularity** | The popularity of a track is a value between 0 and 100.\n",
    "| **duration_ms** | The track length in milliseconds |\n",
    "| **explicit** | **Whether or not the track has explicit lyrics, this column is one hot encoded <br> into 2 columns, for True and False respectively** |\n",
    "| **danceability** | Danceability describes how suitable a track is for dancing |\n",
    "| **energy** | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of <br> intensity and activity\n",
    "| **key** | **The key the track is in. Integers map to pitches using standard Pitch Class notation, <br> this column is one hot encoded into 12 columns, one for each key**|\n",
    "| **loudness** | The overall loudness of a track in decibels (dB) |\n",
    "| **mode** | Mode indicates the modality (major or minor) of a track (major=1, minor=0) |\n",
    "| **speechiness** | Speechiness detects the presence of spoken words in a track |\n",
    "| **acousticness** | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. |\n",
    "| **instrumentalness** | Predicts whether a track contains no vocals |\n",
    "| **liveness** | Detects the presence of an audience in the recording|\n",
    "| **valence** | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track |\n",
    "| **tempo** | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology |\n",
    "| **time_signature** | An estimated time signature. The time signature (meter) is a notational <br> convention to specify how many beats are in each measure. |\n",
    "| **track_genre** | The genre in which the track belongs, this is the class to be predicted |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yx7IUZJzB1MJ"
   },
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pT7BKj8B1MJ"
   },
   "source": [
    "We use the data preprocessed for classification during last session.\n",
    "\n",
    "It's already split into a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gV6OEmVWB1MJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"CoursML_Classification_train.csv\")\n",
    "df_test = pd.read_csv(\"CoursML_Classification_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aa-EWQM7B1MK"
   },
   "outputs": [],
   "source": [
    "# Splitting into X and y variables\n",
    "X_train = df_train.drop(\"genre\", axis=1)\n",
    "y_train = df_train[\"genre\"]\n",
    "\n",
    "X_test = df_test.drop(\"genre\", axis=1)\n",
    "y_test = df_test[\"genre\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUPjJgXyB1MK"
   },
   "source": [
    "**Why do we split the detaset into a train and a test set ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMNHvVibB1MK",
    "outputId": "7c7147f9-0f6d-4668-91ad-b3a8b982c135"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbSAjs57B1MK",
    "outputId": "61b6fbf9-f810-4306-af31-55ba9967de3f"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r04-8_luB1MK"
   },
   "source": [
    "**X_train has numerical features only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUBEqppWB1MK",
    "outputId": "8e58580d-daf5-47ff-c3c1-ee1d140475bb"
   },
   "outputs": [],
   "source": [
    "y_train.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37wwKldxB1MK"
   },
   "source": [
    "### Scikit-Learn library (as you may already know it)\n",
    "\n",
    "**Scikit-learn (Sklearn) is the most useful and robust open-source library for machine learning in Python**. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python.\n",
    "\n",
    "This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib.\n",
    "\n",
    "In this notebook, we will use scikit-learn by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs3utpZ0B1MK"
   },
   "source": [
    "### Reminder on the **biais-variance tradeoff**\n",
    "\n",
    "The bias-variance tradeoff is the balance between the model's ability to fit the training data accurately (low bias) and its ability to generalize to new, unseen data (low variance).\n",
    "\n",
    "- **Bias** refers to the error introduced by approximating a real-world problem, which may be complex, by a simple model. **High bias can lead to underfitting**, where the model is too simplistic and fails to capture the underlying patterns in the data.\n",
    "\n",
    "- **Variance** is the model's sensitivity to small fluctuations in the training data. **High variance can lead to overfitting**, where the model performs well on the training data but fails to generalize to new data because it has essentially memorized the training set.\n",
    "\n",
    "The tradeoff occurs because **decreasing bias often increases variance, and vice versa**. The goal is to find the right balance that minimizes both bias and variance, resulting in a model that generalizes well to new data. This is crucial for creating models that perform well in real-world scenarios. Techniques like cross-validation, regularization, and choosing appropriate **hyperparameters** help manage the bias-variance tradeoff.\n",
    "\n",
    "<center>\n",
    "<img src = https://fr.mathworks.com/discovery/overfitting/_jcr_content/mainParsys/image.adapt.full.medium.svg/1679493630702.svg width=500>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zoxhIcnB1MK"
   },
   "source": [
    "**Overfitting** can be observed by comparing the accuracy on the test set and the accuracy on the training set. A model overfits when the accuracy on the training set is close to 100% but the accuracy on the test set is significantly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJKDGlCBB1MK"
   },
   "source": [
    "### **K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yp5DxxwSB1MK"
   },
   "source": [
    "#### Model overview\n",
    "\n",
    "You must have heard of KNN as it is probably the most intuitive classification algorithm.\n",
    "\n",
    "KNN consists in classifying each new datapoint as the most represented class among its $k$ nearest neighbors, $k$ being a hyperparameter of the model.\n",
    "<center>\n",
    "<img src = https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3SwcOCUyVdGauhHrHvOaLA.png width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUIXc8n4B1MK"
   },
   "source": [
    "#### Hyperparameters\n",
    "- $n\\_neighbors$ : number of neighbors, as we have $6$ classes, $k=7$ ensures to have a defined class.\n",
    "\n",
    "- $weights$ : Weight function used in prediction. Default value is 'uniform'. Possible values:\n",
    "    - ‚Äòuniform‚Äô : uniform weights. All points in each neighborhood are weighted equally.\n",
    "    - ‚Äòdistance‚Äô : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "    \n",
    "\n",
    "- $metric$ : Metric to use for distance computation. Default is ‚Äúminkowski‚Äù (l2). Some possible values :\n",
    "    - ‚Äòmanhattan‚Äô\n",
    "    - ‚Äònan_euclidean‚Äô\n",
    "    - ‚Äòcosine‚Äô\n",
    "\n",
    "We keep distance to its default value as we have numerical features.\n",
    "\n",
    "See [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoqasUvSB1ML"
   },
   "source": [
    "#### Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTv4ghjBB1ML",
    "outputId": "2116b60f-a6dd-4a75-cea2-e1ac87442d60"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvcDXsCQB1ML"
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQBOuY_0B1ML",
    "outputId": "5d2810ef-a364-4058-e25f-06f259d0cd05"
   },
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW39VoFqB1ML"
   },
   "source": [
    "**What would happen if we set a very high number for $n\\_neighbors$ (for example a number close to the size of the training set) ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzkTIfIVB1ML",
    "outputId": "e835b540-4207-4c2a-ea05-960070cbdc85"
   },
   "outputs": [],
   "source": [
    "knn_2 = KNeighborsClassifier(n_neighbors=4000)\n",
    "knn_2.fit(X_train, y_train)\n",
    "knn_2.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw0KGPGKB1ML"
   },
   "source": [
    "### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d5jPl44B1ML"
   },
   "source": [
    "#### Model overview\n",
    "<center>\n",
    "<img src=https://media5.datahacker.rs/2021/01/83.jpg width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixXiyZSAB1ML"
   },
   "source": [
    "Logistic regression is a very know classification algorithm, **used mainly for binary classification** but it can also be used for **multi-class classification**.\n",
    "\n",
    "The logistic regression works as follow :\n",
    "\n",
    "1. Compute a \"linear\" model using randomly initialized weights (the weights being a vector for binary classification and a matrix for multi-class classification)\n",
    "2. Compute the logit of this linear model using sigmoid function for binary classification or softmax function for multi-class classification, softmax being defined as follow:\n",
    "$$ \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)} $$\n",
    "3. Compute the loss using the negative log-likelihood which happens to be a convex function with a minimum.\n",
    "4. Minimise the loss and update the weights by solving the equation :\n",
    "$$\\frac{\\partial \\ell}{\\partial \\beta_i} = 0$$\n",
    "with $\\beta_i$ being either the $i^{th}$ value of the weights vector for binary classification of the $i^{th}$ vector of weights for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycVVuKJ0B1ML"
   },
   "source": [
    "#### Hyperparameters\n",
    "- $max\\_iter$ : Maximum number of iterations taken for the solvers to converge. default=100. We choose $max\\_iter = 200$ as our dataset is relatively large\n",
    "\n",
    "See [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J3jhLIiB1ML"
   },
   "source": [
    "#### Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMHZCs-yB1ML",
    "outputId": "51545f80-0069-4670-86de-aa0fbf39b648"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression(max_iter=200)\n",
    "logistic_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sRiAjCCB1ML"
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FIgE-v1B1ML",
    "outputId": "7a2b5b99-b037-4f1a-b2c6-ad55189c7935"
   },
   "outputs": [],
   "source": [
    "logistic_regression.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1cD_c0tB1MO"
   },
   "source": [
    "### **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlSmOVJQB1MO"
   },
   "source": [
    "#### Model overview\n",
    "\n",
    "A decision tree is also a very know tool for classification (both binary and multi-class).\n",
    "\n",
    "It builds a flowchart-like tree structure where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. It is constructed by recursively splitting the training data into subsets based on the values of the attributes until a stopping criterion is met, such as the maximum depth of the tree or the minimum number of samples required to split a node.\n",
    "\n",
    "During training, the Decision Tree algorithm selects the best attribute to split the data based on a metric such as **entropy** or **Gini impurity**, which measures the level of randomness in the subsets. The goal is to find the attribute that maximizes the **information gain** after the split. For example, using the entropy :\n",
    "\n",
    "Entropy formula for a dataset with $N$ classes is : $$ E = - \\sum_{i=1}^{N}p_{i}\\log_{2}p_i$$\n",
    "\n",
    "The Information Gain formula of a specific split in a tree can defined as follow :\n",
    "$$ IG = E(parent) - \\frac{1}{C}\\sum_{j=1}^{C}E(j)$$\n",
    "with\n",
    "- $C$ the number of children\n",
    "- $j$ the $j^{th}$ child\n",
    "\n",
    "<center>\n",
    "<img src = https://static.javatpoint.com/tutorial/machine-learning/images/decision-tree-classification-algorithm.png width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye7snwwgB1MO"
   },
   "source": [
    "#### Hyperparameters\n",
    "- $criterion$ : The function to measure the quality of a split. Supported criteria are ‚Äúgini‚Äù for the Gini impurity and ‚Äúlog_loss‚Äù and ‚Äúentropy‚Äù both for the Shannon information gain\n",
    "- $max\\_depth$ : max dept of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Z-MmKAqB1MO"
   },
   "source": [
    "See [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zw2zar6_B1MO"
   },
   "source": [
    "#### Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YmF_enIB1MO",
    "outputId": "595114dc-b333-42c6-8748-e9093b5286e2"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Random_state controls the randomness of the estimator\n",
    "# Set this to a fixed value to have exactly the same results for every execution\n",
    "decision_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=8, random_state=0)\n",
    "decision_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N345R_gsB1MO"
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6MlpLRbB1MO",
    "outputId": "f20bc516-2e9b-46d7-e281-2e75bf834d18"
   },
   "outputs": [],
   "source": [
    "decision_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwRMywEeB1MP",
    "outputId": "f94739b4-dcda-4163-b4b3-82df2cefb5ca"
   },
   "outputs": [],
   "source": [
    "decision_tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoKZaPPaB1MP"
   },
   "source": [
    "**What would if we use a very high number for $max\\_depth$ ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsRB0t1rB1MP",
    "outputId": "c468606d-9d61-483b-84ae-3f5d0b76a615"
   },
   "outputs": [],
   "source": [
    "decision_tree_2 = DecisionTreeClassifier(criterion=\"entropy\", max_depth=100)\n",
    "decision_tree_2.fit(X_train, y_train)\n",
    "\n",
    "print(str(decision_tree_2.score(X_train, y_train)))\n",
    "print(str(decision_tree_2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "511yQCtJB1MP"
   },
   "source": [
    "#### Tree Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFrmFjr1B1MP",
    "outputId": "d1212624-2d67-48a3-b59f-170f30df8b00"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "decision_tree_vis = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "decision_tree_vis.fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(decision_tree_vis,\n",
    "                   feature_names=X_train.columns,\n",
    "                   class_names=y_train,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRJsid5uB1MP",
    "outputId": "ba8be4f4-57be-4024-bf60-b16dd6c7f48b"
   },
   "outputs": [],
   "source": [
    "print(str(decision_tree_vis.score(X_train, y_train)))\n",
    "print(str(decision_tree_vis.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCN0iERxB1MP"
   },
   "source": [
    "### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9h5m-AcSB1MP"
   },
   "source": [
    "#### Model overview\n",
    "\n",
    "The training algorithm for random forests applies the general technique of **bootstrap aggregating**, or **bagging** (which a subpart of **ensemble methods**), to tree learners.\n",
    "Given a training set $X = x_1,...,x_n$ with responses $Y = y_1,...,y_n$ , bagging repeatedly (*B* times) selects a random sample with replacement of the training set and fits trees to these samples:\n",
    "\n",
    "For $b = 1,...,B$ :\n",
    "1. Sample, with replacement, training examples from $X, Y$; call these $X_b, Y_b$\n",
    "2. Train a classification or regression tree $f_b$ on $X_b, Y_b$\n",
    "\n",
    "After training, predictions for unseen samples can be made by averaging\n",
    "the predictions from all the individual regression trees on $x'$:\n",
    "\n",
    "$$\\hat{f} = \\frac{1}{B} \\sum_{b=1}^{B} f_b (x')$$\n",
    "\n",
    "or **by taking the plurality vote** in the case of **classification trees**.\n",
    "\n",
    "\n",
    "For example, with a dataset of $250$ rows and $100$ columns :\n",
    "<center>\n",
    "<img src= https://upload.wikimedia.org/wikipedia/commons/e/e3/Random_Forest_Bagging_Illustration.png width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hICdEYOGB1MP"
   },
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "- $n\\_estimators$ : the number of trees in the forest, default = $100$\n",
    "- $criterion$ : The function to measure the quality of a split. Supported criteria are ‚Äúgini‚Äù for the Gini impurity and ‚Äúlog_loss‚Äù and ‚Äúentropy‚Äù both for the Shannon information gain\n",
    "- $max\\_depth$ : max dept of the trees\n",
    "\n",
    "See [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RBO5AvvB1MP"
   },
   "source": [
    "#### Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPdgUNc1B1MP",
    "outputId": "79f9876d-83a8-4297-8a99-ffb6dcda6a04"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(criterion=\"entropy\", n_estimators=150, max_depth=15)\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSEHSVS0B1MP"
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjKl2NdXB1MP",
    "outputId": "14713094-101f-47db-8663-d293b486c935"
   },
   "outputs": [],
   "source": [
    "random_forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0clTtgYB1MP"
   },
   "source": [
    "### **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAbeAtXAB1MP"
   },
   "source": [
    "### Model overview\n",
    "\n",
    "\"XGBoost, which stands for **Extreme Gradient Boosting**, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems.\n",
    "\n",
    "A Gradient Boosting Decision Trees (GBDT) is a decision tree **ensemble learning algorithm** similar to random forest, for classification and regression. Ensemble learning algorithms combine multiple machine learning algorithms to obtain a better model.\n",
    "\n",
    "Both random forest and GBDT build a model consisting of multiple decision trees. **The difference is in how the trees are built and combined**.\n",
    "\n",
    "The term ‚Äúgradient boosting‚Äù comes from the idea of ‚Äúboosting‚Äù or improving a single weak model by combining it with a number of other weak models in order to generate a **collectively strong model**. Gradient boosting is an extension of boosting where the process of additively generating weak models is formalized as a gradient descent algorithm over an objective function. Gradient boosting sets targeted outcomes for the next model in an effort to minimize errors. Targeted outcomes for each case are based on the gradient of the error (hence the name gradient boosting) with respect to the prediction.\n",
    "\n",
    "GBDTs iteratively train **an ensemble of shallow decision trees, with each iteration using the error residuals of the previous model to fit the next model**. The final prediction is a weighted sum of all of the tree predictions. **Random forest ‚Äúbagging‚Äù minimizes the variance and overfitting, while GBDT ‚Äúboosting‚Äù minimizes the bias and underfitting**.\n",
    "\n",
    "XGBoost is a scalable and highly accurate implementation of gradient boosting that pushes the limits of computing power for boosted tree algorithms, being built largely for energizing machine learning model performance and computational speed. With XGBoost, **trees are built in parallel**, instead of sequentially like GBDT. It follows a level-wise strategy, scanning across gradient values and using these partial sums to evaluate the quality of splits at every possible split in the training set.\"\n",
    "\n",
    "source : https://www.nvidia.com/en-us/glossary/data-science/xgboost/\n",
    "\n",
    "<center>\n",
    "<img src = https://www.researchgate.net/profile/Li-Mingtao-2/publication/335483097/figure/fig3/AS:934217085100032@1599746118459/A-general-architecture-of-XGBoost.ppm width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J96ykPsGB1MP"
   },
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "- $booster$ : default = gbtree, booster parameter helps us to choose which booster to use. It helps us to select the type of model to run at each iteration. It has 3 options - gbtree, gblinear or dart. gbtree and dart use tree-based models, while gblinear uses linear models.\n",
    "- $max\\_depth$ : default=6, the maximum depth of a tree. Used to control over-fitting.\n",
    "\n",
    "\n",
    "See [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/parameter.html) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6S--zI-B1MP"
   },
   "source": [
    "#### Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aP1fKP1B1MP"
   },
   "outputs": [],
   "source": [
    "# XGBoost is an open-source library that has to be installed\n",
    "# it follows the same logic as scikit-learn with a Model class and a fit function\n",
    "# XGBoost requires labels to be encoded (it cannot be strings)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbuCTUOdB1MP",
    "outputId": "006c2446-b499-4a46-9560-ff3ed565379c"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgboost_classifier = XGBClassifier()\n",
    "xgboost_classifier.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMFxUqLyB1MQ"
   },
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLeWUocTB1MQ",
    "outputId": "b9fc6807-9b68-49f0-b884-5a5b94bdec98"
   },
   "outputs": [],
   "source": [
    "xgboost_classifier.score(X_test, y_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd0W6jIFB1MQ"
   },
   "source": [
    "### **Classifier evaluation** üëÄ\n",
    "\n",
    "Several metrics can be used to evaluate classifiers. We are going to compare the different models we have executed before using the following ones :\n",
    "- Accuracies\n",
    "- Confusion Matrix\n",
    "- Cross-Validation\n",
    "- Precision, Recall, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkpOGQZmB1MQ"
   },
   "outputs": [],
   "source": [
    "# Let's store all our classifiers in a dictionary\n",
    "classifiers = {\"knn\" : knn,\n",
    "               \"logistic_regression\" : logistic_regression,\n",
    "               \"decision_tree\" : decision_tree,\n",
    "               \"random_forest\" : random_forest,\n",
    "               \"xgboost_classifier\" : xgboost_classifier}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5qRddfdB1MQ"
   },
   "source": [
    "#### Accuracies summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rqt7bDKvB1MQ"
   },
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "for classifier in classifiers:\n",
    "    if classifier == \"xgboost_classifier\":\n",
    "        accuracies[classifier] = classifiers[classifier].score(X_test, y_test_encoded)\n",
    "    else:\n",
    "        accuracies[classifier] = classifiers[classifier].score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn3g11rhB1MQ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot a bar plot with labels\n",
    "def barplot_with_labels(data_dict, y_label, title):\n",
    "    data_dict = dict(sorted(data_dict.items(), key=lambda x:x[1]))\n",
    "    fig, ax = plt.subplots()\n",
    "    bar_container = ax.bar(data_dict.keys(), accuracies.values())\n",
    "    ax.set(ylabel=y_label, title=title)\n",
    "    ax.bar_label(bar_container)\n",
    "    plt.xticks(range(len(data_dict)), data_dict.keys(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRCAJMyaB1MQ",
    "outputId": "80c421bc-9378-4648-8df1-183c8b5a78c3"
   },
   "outputs": [],
   "source": [
    "barplot_with_labels(accuracies, 'Accuracy', 'Accuracies comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJp6yZhEB1MQ"
   },
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "A **confusion matrix** is a specific table layout that allows visualization of the performance of a classification algorithm. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class, or vice versa. The name stems from the fact that it makes it easy to see whether the system is confusing two classes (i.e. commonly mislabeling one as another).\n",
    "\n",
    "We are going to plot the confusion matrix for each classifier to see which class is misleaded depending on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UUhYJQ1B1MQ"
   },
   "source": [
    "##### Confusion matrix plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyB-1q7YB1MQ"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC_0hcS0B1MQ"
   },
   "source": [
    "##### Confusion matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-bR0t9XB1MQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix_classifiers = {\n",
    "    \"knn\" : confusion_matrix(y_test, knn.predict(X_test)),\n",
    "    \"logistic_regression\" : confusion_matrix(y_test, logistic_regression.predict(X_test)),\n",
    "    \"decision_tree\" : confusion_matrix(y_test, decision_tree.predict(X_test)),\n",
    "    \"random_forest\" : confusion_matrix(y_test, random_forest.predict(X_test)),\n",
    "    \"xgboost_classifier\" : confusion_matrix(y_test_encoded, xgboost_classifier.predict(X_test)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GukG6_fmB1MQ",
    "outputId": "0136ece0-dc10-449a-a3f0-1d16f4054787"
   },
   "outputs": [],
   "source": [
    "for classifier in confusion_matrix_classifiers:\n",
    "    plot_confusion_matrix(confusion_matrix_classifiers[classifier], y_test.unique(), title=classifier + \" Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1chordhB1MQ"
   },
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIbMZ7dbB1MQ"
   },
   "source": [
    "Cross-validation, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\n",
    "\n",
    "Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\n",
    "\n",
    "<center>\n",
    "<img src=https://scikit-learn.org/stable/_images/grid_search_cross_validation.png width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ct4REDnvB1MQ",
    "outputId": "aff5a2a4-35a1-44eb-d7c7-f8b9057cb75e"
   },
   "outputs": [],
   "source": [
    "# We can leverage the cross_val_score function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for classifier in classifiers:\n",
    "    if classifier != 'xgboost_classifier':\n",
    "        scores = cross_val_score(classifiers[classifier], X_train, y_train, cv=5)\n",
    "        print(f\"The mean accuracy of {classifier} is {scores.mean()} with a std of {scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-K-VteFB1MQ"
   },
   "source": [
    "Regarding theses accuracies, we can assess that random forest is the best model. XGBoost cross-validation can also be performed but is [slightly more tricky](https://xgboost.readthedocs.io/en/stable/python/examples/cross_validation.html), we can guess that XGBoost is still the best model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyWC6cqoB1MQ"
   },
   "source": [
    "#### Precision, Recall, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikkKg8tSB1MR"
   },
   "source": [
    "Precision, Recall and F1-score are metrics used to evaluate classifiers.\n",
    "<center>\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/1024px-Precisionrecall.svg.png width=300>\n",
    "</center>\n",
    "\n",
    "$F_{1} score$ is the harmonic mean of the precision and recall. It thus symmetrically represents both precision and recall in one metric, the closer to $1$ the better:\n",
    "\n",
    "$$F_{1} = 2 \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$$\n",
    "\n",
    "As our use case is a multi-class classification use case, these metrics have to be adpated. Therefore, in a multi-class setting :\n",
    "\n",
    "$$Precision(class=a) = \\frac{TP(class=a)}{TP(class=a)+FP(class=a)}$$\n",
    "$$Recall(class=a) = \\frac{TP(class=a)}{TP(class=a)+FN(class=a)}$$\n",
    "$$F_{1}(class=a)= 2 \\frac{\\mathrm{Precision(class=a)} \\cdot \\mathrm{Recall(class=a)}}{\\mathrm{Precision(class=a)} + \\mathrm{Recall(class=a)}}$$\n",
    "Then global $F_{1} score$ can bu computed as the average of $F_{1} scores$ for each class :\n",
    "$$F_{1} = \\frac{1}{C}\\sum_{c=1}^{C}F_{1}(class=c)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FbKRSVdB1MR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_scores = {}\n",
    "for classifier in classifiers:\n",
    "    if classifier == \"xgboost_classifier\":\n",
    "        y_pred = classifiers[classifier].predict(X_test)\n",
    "        f_score = f1_score(y_test_encoded, y_pred, average=\"macro\")\n",
    "    else:\n",
    "        y_pred = classifiers[classifier].predict(X_test)\n",
    "        f_score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    f1_scores[classifier] = f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1AFoPClB1MR",
    "outputId": "6f71e7be-7693-4813-fece-8700afbd6eec"
   },
   "outputs": [],
   "source": [
    "barplot_with_labels(f1_scores, 'F1 scores', 'F1 scores comparison')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsy_tqz-B1MR"
   },
   "source": [
    "In our specific use case, the evaluation regarding the $F_1$ score gives the same performance ranking\n",
    "\n",
    "#### F1 Score limits\n",
    "\n",
    "F1 score gives **equal importance to precision and recall**. In practice, **different types of mis-classifications incur different costs**.\n",
    "\n",
    "In other words, the relative importance of precision and recall is an aspect of the problem.\n",
    "\n",
    "Depending on if you are evaluating a classifier dedicated to medicine, to nuclear safety or to food quality testing, **the cost of false positive and false negative are not the same**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z26M5rZNB1MR"
   },
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Machine Learning models often has several complex hyperparameters than can be tricky to choose relevantly.\n",
    "\n",
    "**GridSearch** is an algorithm provided by scikit-learn that allows to empirically select the best hyperpameters in a set of provided hyperparameters for any estimator that has fit and predict functions.\n",
    "\n",
    "See [GridSearchCV documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pf_HeSqvB1MR"
   },
   "source": [
    "##### Hyperparameter tuning of KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENEtYjmtB1MR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Let's create a new KNN classifier using sklearn\n",
    "knn_tuned = KNeighborsClassifier()\n",
    "\n",
    "# List every hyperparameters to be tested\n",
    "k_range = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
    "weights = [\"uniform\", \"distance\"]\n",
    "distances = [\"euclidean\", \"cosine\"]\n",
    "\n",
    "param_grid = {\n",
    "    \"n_neighbors\" : k_range,\n",
    "    \"weights\" : weights,\n",
    "    \"metric\" : distances\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INYNUPJdB1MR",
    "outputId": "e0d94291-faa5-449f-b1a5-5600dbd96b58"
   },
   "outputs": [],
   "source": [
    "# Fitting the model for grid search\n",
    "grid = GridSearchCV(knn_tuned, param_grid, cv=10, scoring='accuracy', return_train_score=False, verbose=1)\n",
    "grid_search=grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itH54tmuB1MR",
    "outputId": "7da00ed8-cc65-4ac0-b658-69001e0909d4"
   },
   "outputs": [],
   "source": [
    "print(\"Best params are \" + str(grid_search.best_params_))\n",
    "accuracy = grid_search.best_score_ * 100\n",
    "print(\"Accuracy for our training dataset with tuning is : {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbwIua8TB1MR",
    "outputId": "1b8f665a-929f-47d2-aec6-a5ea306eb79a"
   },
   "outputs": [],
   "source": [
    "knn_opti = KNeighborsClassifier(metric='cosine', n_neighbors=7, weights=\"distance\")\n",
    "knn_opti.fit(X_train, y_train)\n",
    "print(f\"Accuracy for our testing dataset with tuning is {knn_opti.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9C1dtKFVB1MR"
   },
   "source": [
    "GridSearch usage allowed us to improve significantly the accuracy of our KNN classifier\n",
    "\n",
    "However, GridSearch is a very heavy algorithm, therefore it may be used wisely by smartly selecting the hyperparameters to be tested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0m8rRCvB1MR"
   },
   "source": [
    "## **Regression models to predict the popularity of a track** üìª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BACyipuhB1MR"
   },
   "source": [
    "The second use case of today's session is to present regression models to predict the popularity of a track. Regression differs from classification as it consists in predicting a number instead of a class.\n",
    "\n",
    "As for the classification part, we won't cover every models that exist. This part will also be shorter as the real challenge for regression is explainability and this will be covered in a next section.\n",
    "\n",
    "In this section we will use and lightly deep dive into the following algorithms / models :\n",
    "\n",
    "1. Linear Regression\n",
    "2. KNN regressor\n",
    "3. Decisiton Tree Regressor\n",
    "4. Random Forest Regressor\n",
    "5. XGBoost Regressor\n",
    "\n",
    "As you can see, most of the models we used for classification have a regression setting. For this reason, the theoritical details of already covered algorithms will not be explained again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V316V7dqB1MR"
   },
   "source": [
    "### Dataset description reminder\n",
    "\n",
    "As a reminder, here's the description of each column of the dataset that we are going to use for regression. <br>\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ------------ |\n",
    "| **artists** | The artists' names who performed the track, this column is one hot encoded <br> into 7 columns, one for each 6 top artists and one for an other artists column |\n",
    "| **popularity** | The popularity of a track is a value between 0 and 100. This is the target of our regression\n",
    "| **duration_ms** | The track length in milliseconds |\n",
    "| **explicit** | Whether or not the track has explicit lyrics, this column is one hot encoded <br> into 2 columns, for True and False respectively |\n",
    "| **danceability** | Danceability describes how suitable a track is for dancing |\n",
    "| **energy** | Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of <br> intensity and activity\n",
    "| **key** | The key the track is in. Integers map to pitches using standard Pitch Class notation, <br> this column is one hot encoded into 12 columns, one for each key|\n",
    "| **loudness** | The overall loudness of a track in decibels (dB) |\n",
    "| **mode** | Mode indicates the modality (major or minor) of a track (major=1, minor=0) |\n",
    "| **speechiness** | Speechiness detects the presence of spoken words in a track |\n",
    "| **acousticness** | A confidence measure from 0.0 to 1.0 of whether the track is acoustic. |\n",
    "| **instrumentalness** | Predicts whether a track contains no vocals |\n",
    "| **liveness** | Detects the presence of an audience in the recording|\n",
    "| **valence** | A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track |\n",
    "| **tempo** | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology |\n",
    "| **time_signature** | An estimated time signature. The time signature (meter) is a notational <br> convention to specify how many beats are in each measure. |\n",
    "| **track_genre** | **The genre in which the track belongs, this column is one hot encoded <br> into 6 columns, one for each genre**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq6up8iWB1MR"
   },
   "source": [
    "#### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7ZpVhCIB1MR"
   },
   "outputs": [],
   "source": [
    "df_train_regression = pd.read_csv(\"CoursML_Regression_train.csv\")\n",
    "df_test_regression = pd.read_csv(\"CoursML_Regression_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJbO6CHIB1MR"
   },
   "outputs": [],
   "source": [
    "# Splitting into X and y variables\n",
    "X_train_reg = df_train_regression.drop(\"popularity\", axis=1)\n",
    "y_train_reg = df_train_regression[\"popularity\"]\n",
    "\n",
    "X_test_reg = df_test_regression.drop(\"popularity\", axis=1)\n",
    "y_test_reg = df_test_regression[\"popularity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltZshY6NB1MR"
   },
   "source": [
    "### **Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqAvM4gXB1MR"
   },
   "source": [
    "#### Model overview\n",
    "\n",
    "Linear regression is a **linear approach** for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression.\n",
    "\n",
    "Given a  data set $\\{y_i,\\, x_{i1}, \\ldots, x_{ip}\\}_{i=1}^n$ of $n$ statistical units, a linear regression model assumes that the relationship between the dependent variable $y$ and the vector of regressors $x$ is linear. This relationship is modeled through an \"error variable\" $\\epsilon$ ‚Äî an unobserved random variable that adds \"noise\" to the linear relationship between the dependent variable and regressors. Thus the model takes the form\n",
    " $$y_i = \\beta_{0} + \\beta_{1} x_{i1} + \\cdots + \\beta_{p} x_{ip} + \\varepsilon_i\n",
    " = \\mathbf{x}^\\mathsf{T}_i\\boldsymbol\\beta + \\varepsilon_i,\n",
    " \\qquad i = 1, \\ldots, n, $$\n",
    "where <sup>T</sup> denotes the transpose.\n",
    "\n",
    "Often these $n$ equations are stacked together and written in matrix notation as\n",
    "$$ \\mathbf{y} = \\mathbf{X} \\boldsymbol\\beta + \\boldsymbol\\varepsilon, \\,$$\n",
    "\n",
    "<center>\n",
    "<img src=https://www.reneshbedre.com/assets/posts/reg/reg_front.svg width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-pu0SzNB1MR"
   },
   "source": [
    "Without deepdiving too much into the maths, and because this is a model you probbably already know, here is the optimal linear estimator $\\hat{\\beta}$ obtained using the **least squares methods** :\n",
    "\n",
    "$$\\hat{\\beta} = \\left(X^\\textsf{T}X\\right)^{-1}X^\\textsf{T}Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh693LVdB1MR"
   },
   "source": [
    "#### Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hvkVq-aB1MR",
    "outputId": "4fe1dcec-526c-4e2a-dec4-9eb0ff177538"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ywQpd3KB1MS"
   },
   "source": [
    "#### Model performance\n",
    "\n",
    "We are going to use the 2 following metrics to evaluate the quality of a linear regression :\n",
    "- Mean squared error (MSE) : $\\frac{1}{n} \\sum_{i=1}^n \\left(Y_i-\\hat{Y_i}\\right)^2$ is the average squared difference between the estimated values and the actual value. This value is not bounded, therefore can only measure the quality of one MSE by comparing it to other MSE.\n",
    "- Determination coefficient : $R^2 = 1 - \\frac{\\sum_{i}e_{i}^2}{\\sum_{i}(y_i - \\bar{y})^2}$ is the proportion of the variation in the dependent variable that is predictable from the independent variable(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6KjNy8fB1MS"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Keep r2, MSE, beware of MSE\n",
    "\n",
    "def regression_results(y_true, y_pred):\n",
    "    # Regression metrics\n",
    "    mse=metrics.mean_squared_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MSE: ', round(mse,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sv91sAr5B1MS",
    "outputId": "cde70ea8-7c69-4f1d-8968-385bf700ca5c"
   },
   "outputs": [],
   "source": [
    "y_pred_reg = reg.predict(X_test_reg)\n",
    "regression_results(y_test_reg, y_pred_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sGmOwPKB1MS"
   },
   "source": [
    "$R^2$ is really low, we can conclude that linear regression is not relevant for our use case, let's try other regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJh-he3jB1MS"
   },
   "source": [
    "##### Precision error display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZszGDjBcB1MS"
   },
   "source": [
    "This plot shows how far the predictions of our model are from the real values, we can observe that our model is not good as the points are not close the $y=x$ line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-cg_7ywB1MS",
    "outputId": "95773531-1511-47f0-af9a-2b36ba286493"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "display = PredictionErrorDisplay.from_predictions(\n",
    "    y_test_reg, y_pred_reg, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx7ouBy5B1MS"
   },
   "source": [
    "#### Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hxow2bPAB1MS"
   },
   "source": [
    "Linear regression is a highly criticised model, mainly because **we can fit pretty much anything into a linear regression** and pretend to explain any variable with anything that linearly correlates with it. On one hand, a big risk is to mislead between correlation and causation. On another hand, there's also a high chance that the behaviour one tries to model is not linear.\n",
    "\n",
    "<center>\n",
    "<img src = https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/2560px-Anscombe%27s_quartet_3.svg.png width=500>\n",
    "</center>\n",
    "\n",
    "The ancombe's quartet is a famous dataset which shows how very different datasets can lead to exactly the same linear regression which is not even close to catch the right behaviour occuring in the data. Other takeways from this exemple are :\n",
    "- Visualisation is very important to get an intuition of which model is the most relevant to use\n",
    "- Statistical metrics such as mean, median, variance, etc. are not enough to describe a distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6k37OAKB1MS"
   },
   "source": [
    "### **KNN Regressor**\n",
    "\n",
    "#### Model overview\n",
    "\n",
    "KNN regressor works the same as KNN Classifier, the only difference is that rather than classifying a datapoint with the same class as the k neirest neighbors, the simplest model takes the **average target value** of the k neirest neighbors to predict the target value of a new datapoint.\n",
    "\n",
    "Like the KNN Classifier, more complex versions of the algorithm are using weighted distances and non euclidean metrics to compute the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZ7xPuJOB1MS",
    "outputId": "dd57678e-c365-41b3-da95-e1878f3bb364"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=7, weights='distance')\n",
    "knn_reg.fit(X_train_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VqFz4oNB1MS",
    "outputId": "1a4fdb70-77fe-4e05-a8b5-483c08b99dd1"
   },
   "outputs": [],
   "source": [
    "y_pred_reg = knn_reg.predict(X_test_reg)\n",
    "regression_results(y_test_reg, y_pred_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaXs-iMNB1MS"
   },
   "source": [
    "### **Decision Tree Regressor**\n",
    "\n",
    "The decision tree regressor works exactly the same as the decision tree classifier, the **difference relies on the splitting criterion**, which is a measure of the error / the variance on each node.\n",
    "\n",
    " More precisely for each split, the **$MSE$** is computed, then the spitting value that minimises the **$MSE$** is chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDdMZdUJB1MS"
   },
   "source": [
    "#### Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGJxp2X4B1MS",
    "outputId": "623e0d12-d827-45bd-eb22-84a22f9671ee"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "decision_tree_regressor = DecisionTreeRegressor(max_depth=25)\n",
    "decision_tree_regressor.fit(X_train_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8a564ywB1MS"
   },
   "source": [
    "#### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxlvtYltB1MS",
    "outputId": "8ce15139-c17e-4136-a0bd-9cbc3c6b2f75"
   },
   "outputs": [],
   "source": [
    "y_pred_reg = decision_tree_regressor.predict(X_test_reg)\n",
    "regression_results(y_test_reg, y_pred_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X82R_GFrB1MS"
   },
   "source": [
    "The performance of the decision tree regressor are really poor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--ovaeQoB1MS"
   },
   "source": [
    "### **Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfmW-A7zB1MS"
   },
   "source": [
    "#### Model overview\n",
    "\n",
    "Random Forest regression works the same as the Random Forest classifier but instead of a vote, a mean is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPDqc7TiB1MS",
    "outputId": "76a8e4c3-7612-418c-d417-cfb4323a1e1f"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "random_forest_reg = RandomForestRegressor()\n",
    "random_forest_reg.fit(X_train_reg, y_train_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ghgQ5NfB1MS"
   },
   "source": [
    "#### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlaaN9zUB1MS",
    "outputId": "819c314e-748d-4695-c7d4-aa717c7ed663"
   },
   "outputs": [],
   "source": [
    "y_pred_reg = random_forest_reg.predict(X_test_reg)\n",
    "regression_results(y_test_reg, y_pred_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEIN96kpB1MT"
   },
   "source": [
    "Random forest performs better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HP4ibuOqB1MT"
   },
   "source": [
    "### **XGBoost Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jikG_ZW9B1MT"
   },
   "source": [
    "#### Model overview\n",
    "\n",
    "Same as Random Forest, XGBoost regression works the same as XGBoost classifier it computes the mean of every tree instead of outputing the majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgW_SjBtB1MT",
    "outputId": "6dbe7219-a7de-4c52-de59-a1b688380edf"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "y_train_reg_encoded = label_encoder.fit_transform(y_train_reg)\n",
    "y_test_reg_encoded = label_encoder.fit_transform(y_test_reg)\n",
    "\n",
    "xgboost_regressor = XGBRegressor()\n",
    "xgboost_regressor.fit(X_train_reg, y_train_reg_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkh0SClfB1MT"
   },
   "source": [
    "#### Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIJNvikkB1MT",
    "outputId": "2444ff30-ec6b-4451-b8bd-5a7ff9e8f522"
   },
   "outputs": [],
   "source": [
    "y_pred_reg = xgboost_regressor.predict(X_test_reg)\n",
    "regression_results(y_test_reg_encoded, y_pred_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsj0RbCDB1MT"
   },
   "source": [
    "## Time Series bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDtgG3o0B1MT"
   },
   "source": [
    "\n",
    "**A time series is a sequence of data points collected or recorded over a period of time, typically with equal intervals between each point.**\n",
    "\n",
    "Time series modeling involves the use of statistical and mathematical techniques to analyze and predict patterns within a sequence of data points collected or recorded over time. The goal is to understand the underlying structure of the data, make predictions about future values, and gain insights into the temporal behavior of the phenomena represented by the time series. It is widely used in various fields, including finance, economics, signal processing, and environmental science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eaiu-d4jB1MT"
   },
   "source": [
    "Books:\n",
    "\n",
    "- \"Time Series Analysis and Its Applications: With R Examples\" by Robert H. Shumway and David S. Stoffer\n",
    "- \"Forecasting: Principles and Practice\" by Rob J Hyndman and George Athanasopoulos\n",
    "- \"Introduction to Time Series and Forecasting\" by Peter J. Brockwell and Richard A. Davis\n",
    "\n",
    "Online Courses:\n",
    "\n",
    "- [Practical Time Series Analysis on Coursera by State University of New York](https://www.coursera.org/learn/practical-time-series-analysis)\n",
    "\n",
    "Papers:\n",
    "\n",
    "- \"A comprehensive beginner‚Äôs guide to create a Time Series Forecast (with Codes in Python)\" by Aarshay Jain. [Link to the article](https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/)\n",
    "- \"Time Series Forecasting ‚Äì ARIMA, LSTM, Prophet with Python\" by Aman Dalmia. [Link to the article](https://medium.com/@cdabakoglu/time-series-forecasting-arima-lstm-prophet-with-python-e73a750a9887)\n",
    "\n",
    "Documentation and Tutorials:\n",
    "\n",
    "- [Statsmodels Documentation](https://www.statsmodels.org/stable/index.html) for time series analysis in Python.\n",
    "- [Prophet Documentation](https://facebook.github.io/prophet/) for forecasting time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVU21rB9B1MT"
   },
   "source": [
    "<font size='5'>The course ends here, thank you for listening ! </font><br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv_hi5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
